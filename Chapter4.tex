\chapter{Completeness Theorem for Probabilistic Regular Expressions}
\label{chapter4}
\section{Overview}\label{c4:sec:overview}
In this section, we will introduce the syntax and the language semantics of probabilistic regular expressions ({PRE}), as well as a candidate inference system to reason about the equivalence of {PRE}. 

\subsection{Syntax}\label{s4:subsec:syntax}
Given a finite alphabet $\alphabet$, the syntax of {PRE} is given by:
$$e,f \in \PExp ::= \zero \mid \one \mid a \in A \mid e \oplus_p f \mid e\seq f \mid e^{[p]} \quad\quad\quad p \in\interval{0}{1}$$
We denote the expressions that immediately abort and successfully terminate by $\zero$ and $\one$ respectively. For every letter $a \in \alphabet$ in the alphabet, there is a corresponding expression representing an atomic action. Given two expressions $e, f \in {\PExp}$ and $p \in \interval{0}{1}$, probabilistic choice $e \oplus_p f$ denotes an expression that performs $e$ with probability $p$ and performs $f$ with probability $1-p$. One can think of $ \oplus_p$ as the probabilistic analogue of the
 plus operator ($e + f$) in Kleene's regular expressions. $e \seq f$ represents sequential composition, while $e^{[p]}$ is a probabilistic analogue of Kleene star: it successfully terminates with probability $1-p$ or with probability $p$ performs $e$ and then iterates $e^{[p]}$ again.  In terms of the notational convention, the sequential composition $(\seq)$ has higher precedence than the probabilistic choice $(\oplus_p)$.

\begin{example}
    The expression $a \seq a^{[\frac{1}{4}]}$ first performs action $a$ with probability $1$ and then enters a loop which successfully terminates with probability $\frac{3}{4}$ or performs action $a$ with probability $\frac{1}{4}$ and then repeats the loop again. Intuitively, if we think of the action $a$ as observable, the expression above denotes a probability associated with a non-empty sequence of $a$'s. For example, the sequence $aaa$ would be observed with probability $1 \cdot (1/4)^2 \cdot 3/4=3/64$.
\end{example}
\subsection{Language semantics}\label{c4:subsec:language}
{PRE} denote probabilistic languages $\alphabet^\ast \to \interval{0}{1}$. For instance, the expression $\zero$ denotes a function that assigns $0$ to every word, whereas $\one$ and $a$ respectively assign probability $1$ to the empty word and the word containing a single letter $a$ from the alphabet. The probabilistic choice $e \oplus_p f$ denotes a language in which the probability of each word is the total sum of its probability in $e$ scaled by $p$ and its probability in $f$ scaled by $1-p$. Describing the semantics of sequential composition and loops inductively is more involved. In particular, the semantics of loops would require a fixpoint calculation, which does not have as clear and straightforward (closed-form) formula, as the asterate of regular languages. Instead, we take an \emph{operational approach}, and we formally define the language semantics of {PRE} in \Cref{sec:language_semantics} through a small-step operational semantics, using a specific type of probabilistic transition system, which we introduce next.

\subsection{Generative probabilistic transition systems}\label{c4:subsec:generative}
A {GPTS} consists of a set of states $Q$ and a transition function that maps each state $q\in Q$ to finitely many distinct outgoing arrows of the form:
\begin{itemize}
    \item \emph{successful termination} with probability $t$ (denoted $q \xRightarrow[]{t} \checkmark$), or 
    \item to another state $r$, via an \emph{$a$-labelled transition}, with probability $s \in \interval{0}{1} $ (denoted $q \xrightarrow[]{a \mid s} r$).
\end{itemize} 
We require that, for each state, the total sum of probabilities appearing on outgoing arrows sums up to less or equal to one. The remaining probability mass is used to model unsuccessful termination, hence the state with no outgoing arrows can be thought of as exposing deadlock behaviour. 

Given a word $w \in \alphabet^\ast$ the probability of it being generated by a state $q\in Q$ (denoted $\langmap(q)(w) \in \interval{0}{1}$) is defined inductively:
\begin{equation}\label{c4:eqn:language}
    \langmap(q) (\emptyword)=t \quad\text{if } q \xRightarrow[]{t} \checkmark \qquad\qquad
    \langmap(q)(av) = \sum_{q \xrightarrow[]{a \mid s} r} s \cdot \langmap(r)(v)
\end{equation}
We say that two states $q$ and $q'$ are \emph{language equivalent} if for all words $w \in \alphabet^*$, we have that $\langmap(q)(w)=\langmap(q')(w)$.
\begin{example}\label{c4:ex:systems}
 Consider the following {GPTS}:
 \begin{gather*}
\begin{tikzpicture}[baseline=-3ex]
        \node (0) {$q_0$};
        \node (1) [right= 1.15 cm of 0]{$q_1$};
        \draw (0) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {1}\)} (1);
        \draw (1) edge[loop above] node[left] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (1);
        \node (o1) [right= 0.5 cm of 1]{$\checkmark$};
        \draw (1) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( \frac{3}{4}\)} (o1);
        \node (2) [right= 0.5 cm of o1] {$q_2$};
         \node (2a) [right= 1.25 cm of 2] {};
        \node (3) [below= -.01 cm of 2a] {$q_3$};
        \draw (2) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \draw (3) edge[loop below] node[right] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \node (4) [right= 3 cm of 2] {$q_4$};
        \draw (3) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \draw (2) edge[-latex, bend left] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \node (o2) [right= 0.5 cm of 4]{$\checkmark$};
        \draw (4) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{gather*}
%States $q_0$ and $q_2$ despite not being bisimilar (in the sense from~\cite{Larsen:1991:Bisimulation}), 
States $q_0$ and $q_2$ both assign probability $0$ to the empty word $\emptyword$ and each word $a^{n+1}$ is mapped to the probability $\left(\frac{1}{4}\right)^n \cdot \frac{3}{4}$. Later in the paper, we show that the languages generated by states $q_0$ and $q_2$ can be specified using expressions $a\seq a^{\left[ \frac{1}{4} \right]}$ and $a \oplus_{\frac{3}{4}} ( a\seq a^{\left[ \frac{1}{4} \right]} \seq a)$ respectively.
\end{example}
In \Cref{sec:operational_semantics}, we will associate to each {PRE} $e$ an operational semantics or, more precisely, a state $q_e$ in a {GPTS}. The language semantics $\sem{e}$ of $e$ will then be the language $\langmap( q_e) \colon \alphabet^\ast \to \interval{0}{1}$ generated by $q_e$. Two {PRE} $e$ and $f$ are language equivalent if $\langmap(q_e)=\langmap(q_f)$. One of our main goals is to present a complete inference system to reason about language equivalence. In a nutshell, we want to present a system of (quasi-)equations of the form $e \equiv f$ such that:
\[
e\equiv f \Leftrightarrow \sem {e} = \sem{g} \Leftrightarrow \langmap(e) = \langmap(f)
\]
Such an inference system will have to contain rules to reason about all constructs of {PRE}, including probabilistic choice and loops. We describe next the system, with some intuition for the inclusion of each group of rules. 

\subsection{Axiomatisation of language equivalence of {PRE}}\label{c4:subsec:axiomatisation}
We define ${\equiv} \subseteq \PExp \times \PExp$ to be the least congruence relation closed under the axioms shown on \Cref{c4:fig:axioms}. We will show in \Cref{sec:completeness} that these axioms are complete with respect to language semantics. 
\begin{figure}[h]
\begin{center}	
{
\small
\(
\begin{array}{ll}
&\textbf{Probabilistic choice}\\
(\mathsf{C1})\;\; 
&  e \oplus_p e \equiv e \\
(\mathsf{C2})\;\; 
&  e \oplus_1 f \equiv  e  \\
(\mathsf{C3})\;\; 
&  e \oplus_p f \equiv f \oplus_{1-p} e  \\
(\mathsf{C4})\;\; 
&  (e \oplus_{p} f) \oplus_{q} g \equiv e \oplus_{pq} \left(f \oplus_{\frac{{(1-p)}q}{1-pq}} g\right)  \\\\
&\textbf{Sequential composition}\\
	(\mathsf{1S})\;\; 
&  \one \seq e \equiv_0 e \,, \\
(\mathsf{S})\;\; 
&  e \seq (f \seq g) \equiv_0 (e \seq f) \seq g \,, \\
(\mathsf{S1})\;\; 
&  e \seq \one \equiv_0 e \,, \\
(\mathsf{0S})\;\; 
&  \zero \seq e \equiv_0 \zero \,, \\
(\mathsf{S0})\;\; 
&  e \seq \zero \equiv_0 \zero \,, \\
(\mathsf{D1})\;\; 
&  e \seq (f \oplus_p g) \equiv_0 e \seq f \oplus_p e \seq g  \\
(\mathsf{D2})\;\; 
&  (e \oplus_p f) \seq g \equiv_0 e \seq g \oplus_p f \seq g \\\\

%% recursion
&\textbf{Loops} \\
(\mathsf{Unroll})\;\; &  e^{[p]} \equiv e \seq e^{[p]} \oplus_p \one  \\[1.2ex]
(\mathsf{Tight})\;\; & (e \oplus_p 1)^{[q]}  \equiv e^{\left[\frac{pq}{1-(1-p)q}\right]}  \\[1.2ex]
(\mathsf{Div})\;\; & 1^{[1]}  \equiv 0  \\[1.2ex]
(\mathsf{Unique})\;\; & \inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f} \\\\
%%(\Fix)\;\; & \{ s \equiv_0 t[s / X] \}  s \equiv_0 \rec{X}{t} \,, \text{ for $X$ guarded in $t$} \,, \\
%% distance
\
\end{array}
\)\\
\textbf{Termination conidition: $E \colon \PExp \to \interval{0}{1}$}\\
\fcolorbox{red}{white}{$
        \begin{array}{l}
            \quad E(\one)=1 \qquad E(\zero)=E(a) = 0 \qquad
            E \left( e \oplus_p f \right)=pE(e) + {(1-p)}E(f)\\[1.4ex]
            E(e\seq f)=E(e)E(f)\qquad
            \quad E\left(e^{[p]}\right)=
                \begin{cases}
                0 &  \text{\scriptsize $E(e)=1 \wedge p=1$} \\ 
                \frac{1-p}{1-pE(e)} & \text{\scriptsize otherwise}
                \end{cases}   
                \end{array}$
     }
}
\end{center}	
\caption{Axioms for language equivalence of PRE. The rules involving the division of probabilities are defined only when the denominator is non-zero. The function $E(-)$ provides a termination side condition to the (\textsf{Unique}) fixpoint axiom.}
\label{c4:fig:axioms}
\end{figure}

%\begin{figure*}
%        \centering
%        \begin{multicols}{2}
%        	 \begin{flushleft}\underline{\bf Probabilistic Choice}  	 \end{flushleft}
%        \begin{alignat*}{4}
%		\textbf{(C1)} & \;\, 
%		& e &{} \equiv e \oplus_p e\\
%				\textbf{(C2)} & \;\, 
%		& e &{} \equiv e \oplus_1 f\\
%		\textbf{(C3)} & \;\, 
%		& e \oplus_p  f &{} \equiv f \oplus_{\overline{p}} e\\
%		\textbf{(C4)} & \;\, 
%		& (e \oplus_{p} f) \oplus_{q} g & {} \equiv e \oplus_{pq} (f \oplus_{\frac{{(1-p)}q}{1-pq}} g)\\
%		\textbf{(D1)} & \;\, 
%		& (e \oplus_{p} f) \seq g & {} \equiv e \seq g \oplus_{p} f \seq g\\[-0.25ex]
%		\textbf{(D2)} & \;\, 
%		& e \seq (f \oplus_{p} g) & {} \equiv e \seq g \oplus_{p} e \seq f\\[-0.25ex]\\\\
%        \end{alignat*}
%          \begin{flushleft}   \underline{\bf Sequencing} \end{flushleft}
%        \begin{alignat*}{4}
%        \textbf{(0S)} & \;\, 
%		& 0 \seq e & {} \equiv 0 & \\[-0.25ex]
%		\textbf{(S0)} & \;\, 
%		& e \seq 0 & {} \equiv 0 & \\[-0.25ex]
%		\textbf{(1S)} & \;\, 
%		& 1 \seq e & {} \equiv e & \\[-0.25ex]
%		\textbf{(S1)} & \;\, 
%		& e \seq 1 & {} \equiv e \\[-0.25ex]
%		\textbf{(S)} & \;\, 
%		& e \seq (f \seq g) & {} \equiv (e \seq f ) \seq g \\[-0.25ex]
%        \end{alignat*}
%        \end{multicols}
%        \vspace{-2cm}
%         \begin{multicols}{2}
%     \begin{flushleft}    \underline{\bf Loops} \end{flushleft}
%        \begin{alignat*}{4}
%       	\textbf{(Unroll)} & \;\, 
%		& e \seq e^{[p]} \oplus_p 1& {} \equiv e^{[p]} \\
%		\textbf{(Tight)} & \;\, 
%		& (e \oplus_p 1)^{[q]} \seq 1 & {} \equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]} \\[-0.25ex]
%		\textbf{(Div)} & \;\, 
%		& 1^{[1]} & {} \equiv 0 \\
%%		 \\ \quad\;\;\textbf{(Unique)} \;\, 
%%		\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}
%        \end{alignat*}
%          \begin{flushleft}   \underline{\bf (Unique) fixpoint rule} \end{flushleft}
%		$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$
%\end{multicols}
%        \vspace{-.5cm}
%     \begin{flushleft}    \underline{\bf Termination cond. $E : \PExp \to \interval{0}{1}$} \end{flushleft}
%        \fcolorbox{red}{white}{$
%        \begin{array}{l}
%            \quad E(\one)=1 \qquad E(\zero)=E(a) = 0 \qquad
%            E \left( e \oplus_p f \right)=pE(e) + \ol{p}E(f)\qquad 
%            E(e\seq f)=E(e)E(f)\quad \\[1.4ex]
%            \quad E\left(e^{[p]}\right)=
%                \begin{cases}
%                0 &  \text{\scriptsize $E(e)=1 \wedge p=1$} \\ 
%                \frac{1-p}{1-pE(e)} & \text{\scriptsize otherwise}
%                \end{cases}   
%                \end{array}$
%     }
%     %        \end{tabular}
%      %		\begin{align*}
%%            \textbf{(Unroll)} \quad e^{[p]} &\equiv e \seq e^{[p]}\\
%%            \textbf{(Tight)} \quad (e \oplus_p \one)^{[q]} &\equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]}\\
%%            \one^{[1]} &\equiv \zero &\textbf{(Div)}\\
%%           \omit\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}&&\textbf{(Unique)}\\
%%        \end{align*}
%        \caption{\label{fig:axioms} For $p \in \interval{0}{1}$, we write $\ol{p}=1-p$. The rules involving the division of probabilities are defined only when the denominator is non-zero. The function $E(-)$ provides a termination side condition to the \textbf{Unique} fixpoint axiom.}
%            \vspace{-.4cm}
%    \end{figure*}
%We will also write ${\equiv_0} \subseteq {\PExp \times \PExp}$ for the least congruence relation containing all the above rules except the two marked with $\heartsuit$. Intuitively, removing these rules axiomatises bisimilarity, which is the finer notion of equivalence. This relation will be used in the intermediate step of the proof of soundness. We will use the following notation for the quotient maps:
%% https://q.uiver.app/?q=WzAsMyxbMSwwLCJ7XFxFeHB9L3tcXGVxdWl2XzB9Il0sWzIsMCwie1xcRXhwfS97XFxlcXVpdn0iXSxbMCwwLCJcXEV4cCJdLFswLDEsIlstXV97XFxlcXVpdn0iLDJdLFsyLDAsIlstXV97XFxlcXVpdl8wfSIsMl0sWzIsMSwiWy1dIiwwLHsiY3VydmUiOi0zfV1d
%\[\begin{tikzcd}
%	\PExp & {{\PExp}/{\equiv_0}} & {{\PExp}/{\equiv}}
%	\arrow["{[-]_{\equiv}}"', from=1-2, to=1-3]
%	\arrow["{[-]_{\equiv_0}}"', from=1-1, to=1-2]
%	\arrow["{[-]}", curve={height=-18pt}, from=1-1, to=1-3]
%\end{tikzcd}\]

The first group of axioms capture properties of the probabilistic choice operator $\oplus_p$ (\textsf{C1-C4}) and its interaction with sequential composition (\textsf{D1-D2}). Intuitively, (\textsf{C1-C4}) are the analogue of the semilattice axioms governing the behaviour of $+$ in regular expressions. These four axioms are reminiscent of the axioms of barycentric algebras~\cite{Stone:1949:Postulates}. (\textsf{D1}) and (\textsf{D2}) are \emph{right and left distributivity} rules of $\oplus$ over $\seq$. The sequencing axioms (\textsf{1S}), (\textsf{S1}), (\textsf{S})  state {PRE} have the structure of a monoid (with neutral element $\one$ with absorbent element $\zero$ -- see axioms (\textsf{0S}), (\textsf{S0}). The loop axioms contain respectively \emph{unrolling}, \emph{tightening}, and \emph{divergency} axioms plus a \emph{unique fixpoint} rule. The (\textsf{Unroll}) axiom associates loops with their intuitive behaviour of choosing, at each step, probabilistically between successful termination and executing the loop body once. (\textsf{Tight}) and (\textsf{Div}) are the probabilistic analogues of the identity $(e + \one)^{*} \equiv e^{*}$ from regular expressions. In the case of {PRE}, we need two axioms: (\textsf{Tight}) states that the probabilistic loop whose body might instantly terminate, causing the next loop iteration to be executed immediately is provably equivalent to a different loop, whose body does not contain immediate termination; (\textsf{Div}) takes care of the edge case of a no-exit loop and identifies it with failure. Finally, the unique fixpoint rule is a re-adaptation of the analogous axiom from Salomaa's axiomatisation and provides a partial converse to the loop unrolling axiom, given the loop body is productive -- i.e. cannot immediately terminate. This productivity property is formally written using the side condition $E(e) = 0$, which can be thought of as the probabilistic analogue of empty word property from Salomaa’s axiomatisation. Consider an expression $a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)$. The only way it can accept the empty word is to leave the loop with the probability of $\frac{1}{2}$ and then perform $1$, which also can happen with probability $\frac{1}{2}$. In other words, $\llbracket a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)\rrbracket(\emptyword) = \frac{1}{4}$. A simple calculation allows to verify that $E( a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)) = \frac{1}{4}$.




\begin{example} We revisit the expressions from \Cref{c4:ex:systems} and show their equivalence via axiomatic reasoning.
\begin{align*}
     a \seq a^{[\frac{1}{4}]} &\equiv a \seq (a^{[\frac{1}{4}]} \seq a \oplus_{\frac{1}{4}} \one) \tag{$\dagger$}\\ 
     &\equiv  (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \seq \one \tag{\textsf{D2}}\\
     &\equiv (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \tag{\textsf{S1}}\\
     &\equiv a \oplus_{\frac{3}{4}} (a \seq a^{[\frac{1}{4}]}\seq a)  \tag{\textsf{C3}}
\end{align*}

The $\dagger$ step of the proof above relies on the equivalence $e^{[p]}\seq e \oplus_p \one \equiv e$ derivable from other axioms under the assumption $E(e)=0$ through a following line of reasoning:
\begin{align*}
    e^{[p]} \seq e \oplus_p \one  &\equiv (e \seq e^{[p]} \oplus_p \one)\seq e\oplus_p \one \tag{\textsf{Unroll}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p \one\seq e) \oplus_p \one \tag{\textsf{D1}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p e) \oplus_p \one \tag{\textsf{1S}}\\
        &\equiv (e \seq e^{[p]}\seq e \oplus_p e\seq \one) \oplus_p \one \tag{\textsf{S1}}\\
    &\equiv e \seq (e^{[p]}\seq e \oplus_p \one) \oplus_p \one \tag{\textsf{D2}}
\end{align*}
Since $E(e)=0$,  we then have: 
\(
     e^{[p]} \seq e \oplus_p \one \stackrel{(\textsf{Unique})}\equiv e^{[p]}\seq \one 
     \stackrel{(\textsf{S1})}\equiv e^{[p]} .
\)
\end{example}
\section{Preliminaries}
In this section, we review the main preliminaries for the technical development outlined in the subsequent sections.
\label{c4:sec:preliminaries}
\subsection{Locally finitely presentable categories}\label{c4:subsec:lfp}
$\D$ is a filtered category, if every finite subcategory $\D_0 \hookrightarrow \D$ has a cocone in $\D$. A filtered colimit is a colimit of the diagram $\D \to \C$, where $\D$ is a filtered category. A directed colimit is a colimit of the diagram $\D \to \C$, where $\D$ is a directed poset. We call a functor \emph{finitary} if it preserves filtered colimits. An object $C$ is \emph{finitely presentable (fp)} if the representable functor $\C(C,-) : \C \to \Set$ preserves filtered colimits. Similarly, an object $C$ is \emph{finitely generated (fg)} if the representable functor $\C(C,-) : \C \to \Set$ preserves directed colimits of monos. Every fp object is fg, but the converse does not hold in general. A category $\C$ is \emph{locally finitely presentable (lfp)} if it is cocomplete and there exists a set of finitely presentable objects, such that every object of $\C$ is a filtered colimit of objects from that set. $\Set$ is the prototypical example of a locally finitely presentable category, where finite sets are the fp objects.

\subsection{Monads and their algebras}\label{c4:subsec:monads}
A monad (over the category $\Set$) is a triple $\monadT = (T, \mu, \eta)$ consisting of a functor $T \colon \Set \to \Set$ and two natural transformations: a unit $\eta \colon \Id \Rightarrow T$ and multiplication $\mu \colon T^2 \Rightarrow T$ satisfying $\mu \circ \eta_T = \id_T=  \mu \circ T\eta$ and $\mu \circ \mu_T = \mu \circ T \mu$
A $\monadT$-algebra (also called an Eilenberg-Moore algebra) for a monad $T$ is a pair $(X, h)$ consisting of a set $X \in \Obj(\C)$, called carrier, and a function $h \colon T X \to X$ such that $h \circ \mu_X = h \circ Th$ and $h \circ \eta_X = \id_X$. A $\monadT$-algebra homomorphism between two $T$-algebras $(X,h)$ and $(Y,k)$ is a function 	$f \colon X \to Y$ satisfying $k \circ Tf = f \circ h$.

$\monadT$-algebras and $\monadT$-homomorphisms form a category $\Set^\monadT$. There is a canonical forgetful functor $\forget \colon \Set^\monadT \to \Set$ that takes each $\monadT$-algebra to its carrier. This functor has a left adjoint $X \mapsto (TX, \mu_X \colon T^2 X \to T)$, mapping each set to its free $\monadT$-algebra. If $X$ is finite, then we call $(TX, \mu_X)$ free finitely generated.

Given a function $f \colon X \to Y$, where $Y$ is a carrier of a $\monadT$-algebra $(Y, h)$, there is a unique homomorphism $f^\sharp \colon (TX, \mu_X) \to (Y,h)$ satisfying $f^\sharp \circ \eta_X = f$ that is explicitly given by $f^\sharp = h \circ Tf$.

\subsection{Generalised determinisation}\label{sec:c4:generalised_determinisation}
Language acceptance of nondeterministic automata (NDA) can be captured via determinisation. {NDA} can be viewed as coalgebras for the functor $N = 2 \times {\powf}^\alphabet$, where $\powf$ is the finite powerset monad. Determinisation converts a {NDA} $(X, \beta \colon X \to 2 \times {\powf X}^\alphabet)$ into a deterministic automaton $\left({\powf X}, \beta^{\sharp} \colon {\powf X} \to 2 \times {(\powf X)}^\alphabet \right)$, where for $A \subseteq X$, we define $\beta^{\sharp}(A) = \bigcup_{x \in A} \beta(a)$. Additionally, $\beta^\sharp$ satisfies $\beta^\sharp(\{x\}) = \beta(x)$ for all $x \in X$. A language of the state $x \in X$ of {NDA}, is given by the language accepted by the state $\{x\}$ in the determinised automaton $({\powf X}, \beta^\sharp)$. 


This construction can be generalised~\cite{Silva:2010:Generalizing} to $HT$-coalgebras, where $T \colon \Set \to \Set$ is an underlying functor of finitary monad $\monadT$ and $H \colon \Set \to \Set$ an endofunctor that admits a final coalgebra that can be lifted to the functor $\overline{H} \colon \Set^\monadT \to \Set^\monadT$. Liftings of functors $H \colon \Set \to \Set$ to $\overline{H} \colon \Set^\monadT \to \Set^\monadT$, are in one-to-one correspondence with distributive laws of the monad $\mathsf{T}$ over the functor $H$~\cite{Jacobs:2015:Trace}, which are natural transformations $\rho \colon TH \Rightarrow HT$ satisfying
$H\eta_X = \rho_X \circ \eta_{HX}$ and $H \mu_X \circ \rho_{TX} \circ T\rho_X = \rho_X \circ \mu_{HX}$. In particular, given a $\monadT$-algebra $(X, k \colon TX \to X)$, we can equip $HX$, with a $\monadT$-algebra structure, given by the following composition of maps:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJUSFgiXSxbMiwwLCJIVFgiXSxbNCwwLCJIWCJdLFswLDEsIlxccmhvX1giXSxbMSwyLCJIdCJdXQ==
\[\begin{tikzcd}
	THX && HTX && HX
	\arrow["{\rho_X}", from=1-1, to=1-3]
	\arrow["Ht", from=1-3, to=1-5]
\end{tikzcd}\]


Generalised determinisation turns $HT$-coalgebras $(X, \beta \colon X \to HT X)$ into $H$-coalgebras $(T X , \beta^\sharp \colon T X \to H T X)$, where $\beta^\sharp$ is the unique extension arising from the free-forgetful adjunction between $\Set$ and $\Set^\monadT$. The language of a state $x \in X$ is given by $\beh{\beta^\sharp} \circ \eta_X \colon X \to \nu H$, where $\eta$ is the unit of the monad $\monadT$. Since $\beta^\sharp \colon TX \to HTX$ can be seen as a $\monadT$-algebra homomorphism $(TX, \mu_X) \to \overline{H}(TX, \mu_X)$, each determinisation $(TX, \beta^{\sharp})$ can be viewed as an $\overline{H}$-coalgebra $((TX, \mu_X), \beta^{\sharp})$. The carrier of the final $H$-coalgebra can be canonically equipped with $\monadT$-algebra structure, yielding the final $\overline{H}$-coalgebra. In such a case, the unique final homomorphism from any determinisation (viewed as an $H$-coalgebra) is precisely an underlying function of the final $\overline{H}$-coalgebra homomorphism. 

\subsection{Subdistribution monad}\label{c4:subsec:subdistribution}
 A function $\nu \colon X \to \interval{0}{1}$ is called a subprobability distribution or subdistribution, if it satisfies $\sum_{x \in X} \nu(x) \leq 1$. A subdistribution $\nu$ is \emph{finitely supported}  if the set $\supp(\nu) = \{x \in X \mid \nu(x) > 0\}$ is finite. We use $\distf {X}$ to denote the set of finitely supported subprobability distributions on $X$. The weight of a subdistribution $\nu \colon X \to \interval{0}{1}$ is a total probability of its support:
$$|\nu| = \sum_{x \in X} \nu(x)$$
Given $\nu\in\distf X$ and $Y \subseteq X$, we will write $\nu[Y] = \sum_{x \in Y} \nu(x)$. This sum is well-defined as only finitely many summands have non-zero probability. 
 
  
 Given $x \in X$, its \emph{Dirac} is a subdistribution $\delta_x$ which is given by $\delta_x(y)=1$ only if $x=y$, and $0$ otherwise. We will moreover write $\emptydist \in \distf X$ for a subdistribution with an empty support. It is defined as $\emptydist(x)=0$ for all $x \in X$. When $\nu_1, \nu_2 \colon X \to \interval{0}{1}$ are subprobability distributions and $p \in \interval{0}{1}$, we write $p\nu_1 + (1-p)\nu_2$ for the convex combination of $\nu_1$ and $\nu_2$, which is the probability distribution given by $$(p \nu_1 + (1-p) \nu_2)(x) = p\nu_1(x) + (1-p)\nu_2(x)$$
 for all $x \in X$. Note that this operation preserves finite support. 
 
  $\distf$ is in fact a functor on the category $\Set$, which maps each set $X$ to $\distf X$ and maps each arrow $f \colon X \to Y$ to the function $\distf f \colon \distf X \to \distf Y$ given by $$\distf f (\nu)(x) = \sum_{y \in f^{-1}(x)} \nu(y)$$
   Moreover, $\distf$ also  carries a monad structure with unit  $\eta_X(x) = \delta_x$ and multiplication $\mu_X(\Phi)(x) = \sum_{\varphi \in \distf X }\Phi(\varphi)\varphi(x)$ for $\Phi \in \distf^2 X$. Using the free-forgetful adjunction between $\Set$ and category of $\distf$-algebras, given $f \colon X \to \distf Y$, there exists a unique map $f^\sharp \colon \distf X \to \distf Y$ satisfying $f = f^\sharp \circ \delta$ called the \emph{convex extension of $f$}, and explicitly given by $f^\sharp(\nu)(y) = \sum_{x \in X} \nu(x) f(x)(y)$.
   
\subsection{Positive convex algebras}\label{c4:subsec:positive}
By $\sigpca$ we denote a signature given by
$$\sigpca = \left\{\bigboxplus_{i \in I} p_i \cdot (-)_i \mid I \text{ finite}, \forall i \in I \ldotp p_i \in \interval{0}{1}, \sum_{i \in I} p_i \leq 1\right\} $$
A positive convex algebra is a an algebra for the signature $\sigpca$, that is a pair $\A = \left(X, \sigpca^\A\right)$, where $X$ is the carrier set and $\sigpca^\A$ is a set of interpretation functions $\bigboxplus_{i \in I} p_i \cdot (-)_i \colon X^{|I|} \to X$ satisfying the axioms:
\begin{enumerate}
    \item (Projection) \(\bigboxplus_{i \in I} p_i \cdot x_i = x_j\) if \(p_j=1\)
    \item (Barycenter) \(\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J}{q_{i,j}} \cdot {x_j}\right) = \bigboxplus_{j \in J} \left(\sum_{i \in I} p_i q_{i,j} \right) \cdot x_j\)
\end{enumerate}
In terms of notation, we denote the unary sum by $p_0 \cdot x_0$. Throughout this chapter we will we abuse the notation by writing $$\left(\bigboxplus_{i \in I} p_i \cdot e_i\right) \boxplus \left(\bigboxplus_{i \in J} q_j \cdot f_j\right)$$ for a single sum $\bigboxplus_{k \in I + J} r_k \cdot g_k$, where $r_k = p_k$ and $g_k = e_k$ for $k \in I$ and similarly $r_k = q_k$ and $g_k = f_k$ for $k \in J$. Note that this is well-defined only if $\sum_{i \in I} p_i + \sum_{j \in J} r_j \leq 1$.

The signature of positive convex algebras can be alternatively presented as a family of binary operations, in the following way:

\begin{proposition}\label{c4:prop:binary}
    If  $X$ is a set equipped with a binary operation $\boxplus_{p} : X \times X \to X$ for each $p \in \interval{0}{1}$ and a constant $0_\boxplus \in X$ satisfying for all $x,y,z \in X$ (when defined) the following:
    \begin{gather*}
        x \boxplus_{p} x = x \qquad x \boxplus_{1} y = x \qquad x \boxplus_{p} y = y \boxplus_{1-{p}} x \\ (x \boxplus_{p} y) \boxplus_{q} z = x \boxplus_{pq} \left(y \boxplus_{\frac{(1-p)q}{1-pq}} z\right)
    \end{gather*}
    then $X$ carries the structure of a positive convex algebra. The interpretation of $\boxplus_{i \in I} p_i \cdot (-)_{i}$ is defined inductively by the following
    $$\bigboxplus_{i \in I} p_i \cdot x_i = \begin{cases}
        0_\boxplus & \text{if } I = \emptyset \\
        x_0 & \text{if } p_0 = 1\\
        x_n \boxplus_{p_k} \left(\bigboxplus_{i \in I \setminus \{k\}} \frac{p_i}{1-{p_k}}\cdot x_i  \right) &\text{otherwise, for some } k \in I
    \end{cases} $$
\end{proposition}
%\begin{proof}
%    A straightforward re-adaptation of \cite[Proposition~7]{Bonchi:2017:Power}.
%\end{proof}
Below we state several properties of positive convex algebras, that we will use throughout this chapter.
\begin{proposition}\label{c4:prop:properties_of_positive_convex_algebras}
    Let $I$ be a finite indexed set, and let $\{p_i\}_{i \in I}$ and $\{x_i\}_{i \in I}$ be indexed collections of elements of $\interval{0}{1}$ and $X$ respectively. Then, in any positive convex algebra, the following statements hold:
    \begin{enumerate}
        \item
        $${\bigboxplus_{i \in I} p_i \cdot x_i} = {\bigboxplus_{x \in \bigcup_{i \in I} \{x_i\}} \left(\sum_{x_i = x} p_i\right)\cdot x}$$
        \item Let ${=_R} \subseteq {X \times X}$ be a congruence relation, with $[-]_R \colon X \to X/{=_R}$ being its canonical quotient map. Then, 
        $${\bigboxplus_{i \in I} p_i \cdot x_i} =_R {\bigboxplus_{[x]_R \in \bigcup_{i \in I} \left\{[x_i]_R\right\}} \left(\sum_{x_i =_R x} p_i\right)\cdot x}$$
        \item All terms $\bigboxplus_{i \in I } 0 \cdot x_i $ coincide and are all provably equivalent to the empty convex sum.
        \item If $J \subseteq I$ and $J \supseteq \{i \in I \mid p_i \neq 0\}$, then 
        $$
        \bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{j \in J} p_j \cdot x_j
        $$
        \item Let $\sigma \colon I \to I$ be a permutation of the index set $I$. Then, we have that
        $$
        	\bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{i \in I} p_{\sigma(i)} \cdot x_{\sigma(i)}
        $$
    \end{enumerate}
\end{proposition}
\begin{proof}
    We write $[\Phi]$ to denote Iverson bracket, which is defined to be $1$ if $\Phi$ is true and $0$ otherwise.
    
    For \circlednum{1} we have that
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot x_i &= \bigboxplus_{i \in I} p_i \cdot \left( \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} [x_i = x] \cdot  x \right) &\tag{Projection axiom}\\
        &= \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} \left( \sum_{i \in I} p_i[x_i = x] \right) \cdot x \tag{Barycenter axiom} \\
        &=  \bigboxplus_{x \in \cup_{i \in I} \{x_i\} } \left(\sum_{x_i = x} p_i\right) \cdot x
    \end{align*}
    \circlednum{2} can be shown by picking a representative for each equivalence class and then using \circlednum{1}. For \circlednum{3}, by \cite[Lemma~3.4]{Sokolova:2015:Congruences} we know that all terms $\bigboxplus_{i \in I} 0 \cdot x_i$ coincide. To see that they are provably equivalent to the empty convex sum, observe that
    \begin{align*}
        \bigboxplus_{i \in I} 0 \cdot x_i &= \bigboxplus_{i \in I} 0 \cdot \left(\bigboxplus_{j \in \emptyset} p_j \cdot y_j\right)\tag{\cite[Lemma~3.4]{Sokolova:2015:Congruences}}\\
        &= \bigboxplus_{j \in \emptyset} 0 \cdot y_j \tag{Barycenter axiom}\\
    \end{align*}
    Finally, \circlednum{4} follows from \cite[Lemma~3.4]{Sokolova:2015:Congruences}, while \circlednum{5} was proved in \cite[Proposition~3.1]{Doberkat:2008:Erratum}.
\end{proof}
\begin{lemma}\label{lem:flattening_convex_sums}
    Let $I, J$ be finite index sets, $\left\{p_i\right\}_{i \in I}$, $\{q_{i,j}\}_{(i,j) \in I\times J}$ and $\{x_{i,j}\}_{(i,j) \in I \times J}$ indexed collections such that for all $i \in I$ and $j \in J$, $p_i, q_{i,j} \in \interval{0}{1}$ and $x_{i,j} \in X$. If $X$ carries $\pca$ structure, then:
    $$\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) = \bigboxplus_{(i,j) \in I \times J} p_iq_{i,j} \cdot x_{i,j}$$
\end{lemma}
\begin{proof}
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in \{i\} \times J} q_{k,j} \cdot x_{k,j}\right) \\
        &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in I \times J} [k = i]q_{k,j} \cdot x_{k,j}\right) \tag{\Cref{c4:prop:properties_of_positive_convex_algebras}} \\
        &= \bigboxplus_{(k,j) \in I \times J} \left(\sum_{i \in I} p_i [k=i]q_{k,j} \right) \cdot x_{k,j} \tag{Barycenter axiom} \\
        &= \bigboxplus_{(k,j) \in I \times J} p_k q_{k,j}  \cdot x_{k,j} \\
        &= \bigboxplus_{(i,j) \in I \times J} p_i q_{i,j}  \cdot x_{i,j} \tag{Renaming indices}\\
    \end{align*}
\end{proof}
\begin{lemma}\label{c4:lem:grouping_probabilities}
    Let $I$ be a finite index set, $\{p_i\}_{i \in I}$ and $\{q_i\}_{i \in I}$ indexed collections such that $p_i,q_i \in \interval{0}{1}$ for all $i \in I$, $\sum_{i \in I} p_i + \sum_{i \in I} q_i \leq 1$ and let $\{x_i\}_{i \in I}$ and $\{y_i\}_{i \in I}$ indexed collection such that $x_i, y_i \in X$ for all $i \in I$. If $X$ carries $\pca$ structure, then:
    $$
    \left(\bigboxplus_{i \in I} p_i \cdot x_i\right) \boxplus\left(\bigboxplus_{i \in I} q_i \cdot y_i\right) = \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i \right)
    $$
\end{lemma}
\begin{proof}
    Let $J = \{0,1\}$. Define indexed collections $\{r_{i,j}\}_{(i,j) \in I \times J}$ and $\{z_{i,j}\}_{(i,j) \in I \times J}$, such that $r_{i,0} = \frac{p_i}{p_i + q_i}$ and $z_{i,0} = x_i$ and $r_{i,1} = \frac{q_i}{p_i + q_i}$ and $z_{i,1} = x_i$.
    We have the following:
    \begin{align*}
        \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i\right) &= \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\bigboxplus_{j \in J} r_{i,j} \cdot z_{i,j} \right) \\
        &= \bigboxplus_{(i,j) \in I \times J} (p_i + q_i)r_{i_j} \cdot z_{i,j} \tag{\Cref{lem:flattening_convex_sums}} \\
        &= \left(\bigboxplus_{i \in I} p_i \cdot x_{i}\right) \boxplus \left(\bigboxplus_{i \in I} q_i \cdot y_i\right)
    \end{align*}
\end{proof}
Speaking more abstractly, positive convex algebras and their homomorphisms (in the sense of homomorphisms of algebras for the signature from universal algebra) form a category, that we will call $\pca$. This category can be seen as a concrete presentation of an Eilenberg-Moore algebra for the subdistribution monad.

\begin{theorem}\label{c4:thm:correspondence}
	There is an isomorphism of categories between $\pca$ and $\Set^{\distf}$. Given a set $X$ equipped with a positive convex algebra structure, we can define a map $h \colon \distf X \to X$, given by 
	$$
		h(\nu) = \bigboxplus_{x \in \supp(\nu)} \nu(x) \cdot x
	$$
	for all $\nu \in \distf X$, making $(X, h)$ into an algebra for the monad $\distf$. Equivalently, given a $\distf$-algebra $(X, h)$, one can define 
	$$
		\bigboxplus_{i \in I} p_i \cdot x_i = h \left(\sum_{i \in I} p_i \cdot \delta_{x_i}\right)
	$$
	for all finite $I$ and indexed collections $\{p_i\}_{i \in I}$, $\{x_i\}_{i \in I}$, such that $\sum_{i \in I} p_i \leq 1$ and for all $i \in I$, $x_i \in X$. This equips the set $X$ with a positive convex algebra structure.  
	\end{theorem}
	\begin{proof}
		See \cite[Theorem~4]{Jacobs:2010:Convexity} or \cite[Proposition~5.3]{Doberkat:2008:Erratum}.
	\end{proof}
	Moreover, $\pca$ as a category enjoys the following property: 
	\begin{theorem}[{\cite{Sokolova:2015:Congruences}}]
		In $\pca$ finitely presented and finitely generated objects coincide.
	\end{theorem}
	\subsection{Rational fixpoint}\label{c4:subsec:rational}
	Let $B \colon \C \to \C$ be a finitary functor. We write $\coaf{B}$ for the subcategory of $\coa{B}$ consisting only of $B$-coalgebras with finitely presentable carrier. The \emph{rational fixpoint} is defined as $(\varrho B,r) = \colim(\coaf{B} \hookrightarrow \coa{B})$ -- a colimit of the inclusion functor from the subcategory of coalgebras with finitely presentable carriers. We call it a fixpoint, as the map $r \colon \varrho B \to B (\varrho B) $ is an isomorphism~\cite{Adamek:2006:Iterative}. 
% The rational fixpoint can be both viewed as a final locally finitely presentable $F$-coalgebra and an initial iterative $F$-algebra~\cite{Adamek:2006:Iterative}. 
Following~{\cite[Corollary~3.10, Theorem~3.12]{Milius:2020:New}}, if finitely presentable and finitely generated objects coincide in $\C$ and $B : \C \to \C$ is a finitary endofunctor preserving non-empty monomorphisms, then rational fixpoint is fully abstract - ie. $(\varrho B, r)$ is a subcoalgebra of the final coalgebra $(\nu B, t)$.

\section{Operational semantics}\label{c4:sec:operational}

\subsection{Language semantics of GPTS}\label{c4:subsec:language_semantics}
Let $\funF \colon \Set \to \Set$ be an endofunctor given by $\funF = \{\checkmark\} + \alphabet \times (-)$. GPTS are precisely $\distf \funF$-coalgebras, that is pairs $(X, \beta)$, where $X$ is a set of states and $\beta \colon X \to \distf (\{\checkmark\} + \alphabet \times X)$ is a transition structure. Because of this, we will interchangeably use terms "$\distf \funF$-coalgebra" and "GPTS".

The functor $\distf \funF$ admits a final coalgebra, but unfortunately it is not carried by the set of probabilistic languages, that is $\interval{0}{1}^{\alphabet^{\ast}}$, because the canonical semantics of $\distf \funF$-coalgebras happens to correspond to the more restrictive notion of probabilistic bisimilarity (also known as Larsen-Skou bisimilarity~\cite{Larsen:1991:Bisimulation}). Probabilistic bisimilarity is a branching-time notion of equivalence, requiring observable behaviour of compared states to be equivalent at every step, while probabilistic language equivalence is a more liberal notion comparing sequences of observable behaviour. In general, if two states are bisimilar, then they are language equivalent, but the converse does not hold.
\begin{example}\label{c4:ex:bisimilarity}
 Consider the following {GPTS}:
 \begin{gather*}
\begin{tikzpicture}[baseline=-3ex]
        \node (0) {$q_0$};
        \node (1) [right= 1.25 cm of 0]{$q_1$};
        \node (2) [right= 1.25 cm of 1]{$q_2$};
        \node (o1) [right= 0.75 cm of 2]{$\checkmark$};
        \node (3) [right = 1.8 cm of o1]{$q_3$};
        \node (4) [right= 1.25 cm of 3]{$q_4$};
        \node (5) [right= 1.25 cm of 4]{$q_5$};
        \node (o2) [right= 0.75 cm of 5]{$\checkmark$};
        \draw (0) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{2}{3}}\)} (1);
		\draw (1) edge[-latex] node[ fill=white] {\scriptsize\( {b} \mid {\frac{1}{2}}\)} (2);
		\draw (2) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o1);
		\draw (3) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{1}{2}}\)} (4);
		\draw (4) edge[-latex] node[ fill=white] {\scriptsize\( {b} \mid {\frac{2}{3}}\)} (5);
		\draw (5) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{gather*}
States $q_0$ and $q_3$ are language equivalent because they both accept the string $ab$ with the probability $\frac{1}{3}$, but are not bisimilar, because the state $q_0$ can make $a$ transition with the probability $\frac{2}{3}$, while $q_3$ can perform an $a$ transition with probability $\frac{1}{2}$.
\end{example}
A similar situation happens when looking at nondeterministic automata through the lenses of universal coalgebra, where again the canonical notion of equivalence is the one of bisimilarity. A known remedy is the powerset construction from classic automata theory, which converts a nondeterministic automaton to a deterministic automaton, whose states are sets of states of the original nondeterministic automaton we have started from. In such a case, the nondeterministic branching structure is factored into the state space of the determinised automaton. The language of an arbitrary state of the nondeterministic automaton corresponds to the language of the singleton set containing that state in the determinised automaton.

Generalised determinisation extends the above idea to $HT$-coalgebras, where $T \colon \Set \to \Set$ is an underlying functor of a finitary monad $\monadT$ and $H \colon \Set \to \Set$ is a functor that can be lifted to category $\Set^\monadT$ of $\monadT$-algebras. Generalised determinisation provides a uniform treatment of language semantics of variety of transition systems, where the final $H$-coalgebra provides a notion of language. Unfortunately, $\distf \funF$-coalgebras do not fit immediately to this picture.  Luckily, each such $\distf \funF$-coalgebra can be seen as a special case of a more general kind of transition system, known as reactive probabilistic transition systems~({RPTS})~\cite{Glabbeek:1995:Reactive} or Rabin probabilistic automata~\cite{Rabin:1963:Probabilistic}. 



{RPTS} can be intuitively viewed as a probabilistic counterpart of nondeterministic automata and they can be determinised to obtain probabilistic language semantics. In an {RPTS}, each state $x$ is mapped to a pair $\langle o_x,n_x \rangle$, where $o \in \interval{0}{1}$ is the acceptance probability of state $x$ and $n_x\colon \alphabet \to \distf(X)$ is the next-state function, which takes a letter $a \in \alphabet$ and returns the subprobability distribution over successor states. Formally speaking, let $\funG \colon \Set \to \Set$ be an endofunctor $\funG = [0,1] \times (-)^\alphabet$. RPTS are precisely $\funG\distf$-coalgebras, that is pairs $(X, \beta)$, where $X$ is a set of states and $\beta \colon X \to [0,1] \times \distf (X)^\alphabet$ is a transition function. Following the convention outlined before, we will use terms "$G\distf$-coalgebras" and "GPTS" interchangeably.

$\funG \distf$-coalgebras fit into framework of generalised determinisation~\cite{Silva:2011:Sound}. In particular, there exists a distributive law $\rho \colon \distf \funG \Rightarrow \funG \distf$ of the monad $\distf$ over the functor $\funG$ that allows to lift $\funG \colon \Set \to \Set$ to $\funG \colon \pca \to \pca$. Speaking in concrete terms, if the set $X$ is equipped with a convex sum operation $\bigboxplus_{i \in I} p_i \cdot (-)$, then so is $\interval{0}{1} \times X^\alphabet$. Let $\{\langle o_i , t_i\rangle\}_{i \in I}$ be an indexed collection of elements of $\interval{0}{1} \times X^\alphabet$. Then, we can define
$$
\bigboxplus_{i \in I} p_i \cdot \langle o_i, t_i \rangle = \left\langle \sum_{i \in I} p_i\cdot o_i, \lambda a\ldotp \bigboxplus_{i \in I} p_i \cdot t_i(a) \right\rangle
$$
The final coalgebra for the functor $\funG$ is precisely carried by the set $\interval{0}{1}$ of probabilistic languages.

In order to talk about language semantics of $\distf \funF$-coalgebras, we first provide an informal intuition that each $\distf \funF$-coalgebra can be seen as a special case of a $\funG \distf$-coalgebra.
\begin{example}\label{ex:converting}
Fragment of a {GPTS} (on the left) and of the corresponding RPTS (on the right).
 \begin{gather*}
\begin{tikzpicture}[baseline=-3ex]
        \node (o1) {$\checkmark$};
        \node (0) [right= 0.5 cm of o1]{$q_0$};
        \draw (0) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\(\frac{1}{4}\)} (o1);
        \node (1) [below left= .9 cm of 0]{$q_1$};
        \node (2) [below right= .9 cm of 0]{$q_2$};
        \draw (0) edge[-latex] node[above, pos=0.6] {\scriptsize\( {a} \mid {\frac{1}{4}}\qquad  \)} (1);
        \draw (0) edge[-latex] node[above, pos=0.6] {\scriptsize\(\quad {a} \mid {\frac{1}{2}}\)} (2);
        \node (o2) [right= 2cm of 0]{$\frac{1}{4}$};
        \node (3) [right= 0.5 cm of o2]{$q_0$};
        \draw (3) edge[-implies, double, double distance=0.5mm] (o2);
        \node (4) [right= 0.75 cm of 3]{$\circ$};
        \draw (3) edge[-latex] node[above] {\scriptsize\( {a}\)} (4);
        \node (5) [below left= 0.9 cm of 4]{$q_1$};
        \node (6) [below right= 0.9 cm of 4]{$q_2$};
        \draw (4) edge[-latex, dashed] node[left, pos = 0.1] {\scriptsize\( {\frac{1}{4}}\quad\)} (5);
        \draw (4) edge[-latex, dashed] node[right, pos = 0.1] {\scriptsize\(\quad {\frac{1}{2}}\)} (6);
%        \node (2) [right= 1.25 cm of 1]{$q_2$};
%        \node (o1) [right= 0.75 cm of 2]{$\checkmark$};
%        \node (3) [below = 0.6 cm of 0]{$q_3$};
%        \node (4) [right= 1.25 cm of 3]{$q_4$};
%        \node (5) [right= 1.25 cm of 4]{$q_5$};
%        \node (o2) [right= 0.75 cm of 5]{$\checkmark$};
%        \draw (0) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{2}{3}}\)} (1);
%		\draw (1) edge[-latex] node[ fill=white] {\scriptsize\( {b} \mid {\frac{1}{2}}\)} (2);
%		\draw (2) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o1);
%		\draw (3) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{1}{2}}\)} (4);
%		\draw (4) edge[-latex] node[ fill=white] {\scriptsize\( {b} \mid {\frac{2}{3}}\)} (5);
\end{tikzpicture}
\end{gather*}
In the corresponding {RPTS} state $q_0$ accepts with probability $\frac{1}{4}$ and given input $a$ it transitions to subprobability distribution that has $\frac{1}{4}$ probability of going to to $q_1$ and $\frac{1}{2}$ probability of going to $q_2$. \end{example}

We can make the above intuition formal. Let $X$ be a set, and let $\zeta \in \distf \funF X$. Define a function $\gamma_X \colon \distf \funF X \to \funG \distf X$, given by
$$
\gamma_X(\zeta) = \langle \zeta(\checkmark), \lambda a \ldotp \lambda x\ldotp \zeta(a,x)\rangle
$$
Such functions define components of the natural transformation.
\begin{proposition}{\cite{Silva:2011:Sound}}
	$\gamma \colon \distf \funF \Rightarrow \funG \distf$ is a natural transformation with injective components. 
\end{proposition}
We now have all ingredients to specify language semantics of $\distf \funF$-coalgebras. Given a $\distf \funF$-coalgebra  $(X, \beta)$, one can use the natural transformation $\gamma$ and obtain $\funG \distf$-coalgebra $(X, \gamma_X \circ \beta)$. Since $\funG$ can be lifted to $\pca$, we can obtain $\funG$-coalgebra $\left(\distf X, (\gamma_X \circ \beta)^\sharp\right)$. Note that this coalgebra carries an additional algebra structure and its transition map is a $\pca$ homomorphism, thus making $\left( (\distf X, \mu_X), (\gamma_X \circ \beta)^\sharp \right)$ into a $\overline{\funG}$-coalgebra. The resulting language semantics of $(X, \beta)$ are given by the map $\langmap_{(X, \beta)} \colon X \to \interval{0}{1}^\alphabet$ given by $$\langmap_{(X, \beta)} = \beh{(\gamma_X \circ \beta)^\sharp} \circ \eta_X$$ where $\eta \colon X \to \distf X$ is a unit of the monad $\distf$ taking each state $x \in X$ to its Dirac $\delta_x \in \distf X$.

This can be summarised by the following commutative diagram:
% https://q.uiver.app/#q=WzAsNixbMCwwLCJYIl0sWzAsMSwiXFxkaXN0ZiBcXGZ1bkYgWCJdLFsxLDAsIlxcZGlzdGYgWCJdLFswLDIsIlxcZnVuRyBcXGRpc3RmIFgiXSxbMywwLCJcXGludGVydmFsezB9ezF9XlxcYWxwaGFiZXQiXSxbMywyLCJcXGZ1bkcgXFxsZWZ0KFxcaW50ZXJ2YWx7MH17MX1ee1xcYWxwaGFiZXR9XFxyZ2l0aCkiXSxbMCwxLCJcXGJldGEiXSxbMSwzLCJcXGdhbW1hX1giXSxbMCwyLCJcXGV0YV9YIl0sWzIsMywiKFxcZ2FtbWFfWCBcXGNpcmMgXFxiZXRhKV5cXHNoYXJwIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzQsNSwidCJdLFsyLDQsIlxcYmVoeyhcXGdhbW1hX1ggXFxjaXJjIFxcYmV0YSleXFxzaGFycH0iLDAseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJkYXNoZWQifX19XSxbMyw1LCJcXGZ1bkcgXFxiZWh7KFxcZ2FtbWFfWCBcXGNpcmMgXFxiZXRhKV5cXHNoYXJwfSIsMCx7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dXQ==
\[\begin{tikzcd}
	X & {\distf X} && {\interval{0}{1}^{\alphabet^\ast}} \\
	{\distf \funF X} \\
	{\funG \distf X} &&& {\funG \left(\interval{0}{1}^{\alphabet^\ast}\right)}
	\arrow["{\eta_X}", from=1-1, to=1-2]
	\arrow["\beta", from=1-1, to=2-1]
	\arrow["{\beh{(\gamma_X \circ \beta)^\sharp}}", dashed, from=1-2, to=1-4]
	\arrow["{(\gamma_X \circ \beta)^\sharp}", dashed, from=1-2, to=3-1]
	\arrow["t", from=1-4, to=3-4]
	\arrow["{\gamma_X}", from=2-1, to=3-1]
	\arrow["{\funG \beh{(\gamma_X \circ \beta)^\sharp}}", dashed, from=3-1, to=3-4]
\end{tikzcd}\]

The language semantics defined above coincide with the explicit definition of $\langmap$ we gave in \Cref{c4:eqn:language} (this is a consequence of a result in~\cite{Silva:2011:Sound}). 



Moreover, the natural transformation $\gamma \colon \distf \funF \Rightarrow \funG \distf$ interacts well with the distributive law $\rho \colon \distf \funG \Rightarrow \funG \distf$, making the following diagram commute:
% https://q.uiver.app/#q=WzAsNSxbMCwwLCJcXGRpc3RmXjIgXFxmdW5GIl0sWzAsMSwiXFxkaXN0ZiBcXGZ1bkcgXFxkaXN0ZiJdLFsyLDEsIlxcZnVuRyBcXGRpc3RmXjIiXSxbNCwxLCJcXGZ1bkdcXGRpc3RmIl0sWzQsMCwiXFxkaXN0ZlxcZnVuRiJdLFswLDEsIlxcZGlzdGYgXFxnYW1tYSIsMl0sWzEsMiwiXFxyaG9fXFxkaXN0ZiIsMl0sWzIsMywiXFxmdW5HIFxcbXUiLDJdLFs0LDMsIlxcZ2FtbWEiXSxbMCw0LCJcXG11X1xcZnVuRyJdXQ==
\[\begin{tikzcd}
	{\distf^2 \funF} &&&& {\distf\funF} \\
	{\distf \funG \distf} && {\funG \distf^2} && {\funG\distf}
	\arrow["{\mu_\funG}", from=1-1, to=1-5]
	\arrow["{\distf \gamma}"', from=1-1, to=2-1]
	\arrow["\gamma", from=1-5, to=2-5]
	\arrow["{\rho_\distf}"', from=2-1, to=2-3]
	\arrow["{\funG \mu}"', from=2-3, to=2-5]
\end{tikzcd}\]
The above is a consequence of $\gamma \colon \distf \funF \Rightarrow \funG \distf$ being so-called extension law -- for more details see~\cite[Section~7.2]{Jacobs:2015:Trace}. Using this fact, we can show the following:
\begin{proposition}
	For any $\distf \funF$-coalgebra $(X, \beta)$, the following diagram commutes:
	% https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXGRpc3RmIFgiXSxbMiwwLCJcXGRpc3RmXjJcXGZ1bkYgWCJdLFs0LDAsIlxcZGlzdGYgXFxmdW5GIFgiXSxbNiwwLCJcXGZ1bkcgXFxkaXN0ZiBYIl0sWzAsMSwiXFxkaXN0ZiBcXGJldGEiXSxbMSwyLCJcXG11X1xcZnVuRiJdLFsyLDMsIlxcZ2FtbWFfWCJdLFswLDMsIihcXGdhbW1hX1ggXFxjaXJjIFxcYmV0YSleXFxzaGFycCIsMCx7ImN1cnZlIjotNX1dXQ==
\[\begin{tikzcd}
	{\distf X} && {\distf^2\funF X} && {\distf \funF X} && {\funG \distf X}
	\arrow["{\distf \beta}", from=1-1, to=1-3]
	\arrow["{(\gamma_X \circ \beta)^\sharp}", curve={height=-30pt}, from=1-1, to=1-7]
	\arrow["{\mu_{\funF X}}", from=1-3, to=1-5]
	\arrow["{\gamma_X}", from=1-5, to=1-7]
\end{tikzcd}\]
\end{proposition}
\begin{proof}
	We first argue that $ (\gamma_X \circ \beta)^\sharp \circ \eta_X =  \gamma_X \circ \mu_{\funF X} \circ \distf \beta \circ \eta_X  $
	\begin{align*}
		\eta_X \circ \distf \beta \circ \mu_{\funF X} \circ \gamma_X &= \beta\circ \eta_{\distf \funF X} \circ \mu_{\funF X} \circ \gamma_X \tag{$\eta$ is natural}\\
		&= \beta \circ \gamma_X \tag{Monad laws}\\
		&= (\beta \circ \gamma_X)^\sharp \circ \eta_X \tag{Kleisli extension} 
	\end{align*}
	Then, we argue that $\gamma_X \circ \mu_{\funF X} \circ \distf \beta$ is a $\pca$ homomorphism from the free $\pca$ $(X, \mu_X)$ to $\overline{G}(X, \mu_X)$, by checking the commutativity of the diagram below.
	% https://q.uiver.app/#q=WzAsOSxbMCwyLCJcXGRpc3RmIFgiXSxbMCwwLCJcXGRpc3RmXjJYIl0sWzIsMiwiXFxkaXN0Zl4yXFxmdW5GIFgiXSxbMiwwLCJcXGRpc3RmXjMgXFxmdW5GIFxcYmV0YSJdLFs0LDIsIlxcZGlzdGYgXFxmdW5GIFgiXSxbNCwwLCJcXGRpc3RmXjIgXFxmdW5GIFgiXSxbNiwyLCJcXGZ1bkcgXFxkaXN0ZiBYIl0sWzYsMCwiXFxkaXN0ZiBcXGZ1bkYgXFxkaXN0ZlgiXSxbNiwxLCJcXGZ1bkdeMiBcXGRpc3RmIFgiXSxbMSwwLCJcXG11X1giLDJdLFswLDIsIlxcZGlzdGYgXFxiZXRhIiwyXSxbMSwzLCJcXGRpc3RmXjIgXFxiZXRhIl0sWzMsMiwiXFxtdV97XFxkaXN0ZiBcXGZ1bkYgWH0iXSxbMiw0LCJcXG11X3tcXGZ1bkYgWH0iXSxbNSw0LCJcXG11X3tcXGRpc3RmIFxcZnVuRiBYfSIsMl0sWzMsNSwiXFxkaXN0ZiBcXG11X3tcXGZ1bkYgWH0iXSxbNCw2LCJcXGdhbW1hX1giXSxbNSw3LCJcXGRpc3RmIFxcZ2FtbWFfWCJdLFs3LDgsIlxccmhvX3tcXGRpc3RmIFh9Il0sWzgsNiwiXFxmdW5HIFxcbXVfWCJdXQ==
\[\begin{tikzcd}
	{\distf^2X} && {\distf^3 \funF X} && {\distf^2 \funF X} && {\distf \funF \distf X} \\
	&&&&&& {\funG^2 \distf X} \\
	{\distf X} && {\distf^2\funF X} && {\distf \funF X} && {\funG \distf X}
	\arrow["{\distf^2 \beta}", from=1-1, to=1-3]
	\arrow["{\mu_X}"', from=1-1, to=3-1]
	\arrow["{\distf \mu_{\funF X}}", from=1-3, to=1-5]
	\arrow["{\mu_{\distf \funF X}}", from=1-3, to=3-3]
	\arrow["{\distf \gamma_X}", from=1-5, to=1-7]
	\arrow["{\mu_{\distf \funF X}}"', from=1-5, to=3-5]
	\arrow["{\rho_{\distf X}}", from=1-7, to=2-7]
	\arrow["{\funG \mu_X}", from=2-7, to=3-7]
	\arrow["{\distf \beta}"', from=3-1, to=3-3]
	\arrow["{\mu_{\funF X}}", from=3-3, to=3-5]
	\arrow["{\gamma_X}", from=3-5, to=3-7]
\end{tikzcd}\]
The left diagram commutes because $\mu$ is natural, while the middle one commutes because of $\mu$ being a multiplication map of the monad. Finally, the commutativity of the rightmost subdiagram is guaranteed by $\gamma$ being an extension law (see discussion above).

Since, $\mu_{\funF X} \circ \distf \beta \circ \eta_X $ is a $\pca$ homomorphism that factorises through $\eta$ in the same way as $(\gamma_X \circ \beta)^\sharp $, we have that  $ (\gamma_X \circ \beta)^\sharp =  \gamma_X \circ \mu_{\funF X} \circ \distf \beta$.
\end{proof}
\subsection{Antimirov derivatives}\label{c4:subsec:antimirov}
We now equip $\PExp$ with a $\distf \funF$-coalgebra structure, that is, we define a function $\partial : \PExp \to \distf (1+A\times \PExp)$. We refer to $\partial$ as the \emph{Antimirov derivative}, as it is reminiscent of the analogous construction
for regular expressions and nondeterminisic automata due to Antimirov~\cite{Antimirov:1996:Partial}. Given $a \in A$, $e,f \in \PExp$ and $p \in \interval{0}{1}$ we define:
\begin{gather*}
    \partial(\zero) = \emptydist \quad\quad \partial(\one)=\delta_{\checkmark} \quad\quad \partial(a)=\delta_{(a,\one)}  \\  \partial(e \oplus_p f)=p\partial(e) + (1-p)\partial(f)
\end{gather*}
The expression $\zero$ is mapped to the emptysubdistribution, intuitively representing a deadlock. On the other hand, the expression $\one$ represents immediate acceptance, that is it transitions to $\checkmark$ with probability $1$. For any letter $a \in A$ in the alphabet, the expression $a$ performs $a$-labelled transition to $\one$ with probability $1$. The outgoing transitions of the probabilistic choice $e \oplus_p f$ consist of the outgoing transitions of $e$ with
probabilities scaled by $p$ and the outgoing transitions of $f$ scaled by $1-p$.

The definition of $\partial(e;f)$ is slightly more involved. We need to factor in the possibility that $e$ may accept with some probability $t$, in which case the outgoing transitions of $f$ contribute to the outgoing transitions of $e \seq f$. Formally, $\partial(e \seq f) = \partial(e) \lhd f$ where for any $f \in \PExp$ the operation $( - \lhd f) : \distf (1+A\times \PExp) \to \distf (1+A\times \PExp)$ is given by $( - \lhd f) = {c_f}^{\star}$, the convex extension of $c_f : 1+A\times \PExp \to \distf (1+A \times \PExp)$ given below on the left.
\begin{gather*}
c_f(x) = \begin{cases}
    \partial(f) & x = \checkmark \\
    \delta_{(a, e' \seq f)} & x = (a,e')
\end{cases}\qquad\qquad
\begin{tikzpicture}[baseline=-5ex]
        \node (0) {${e}\seq{f}$};
        \node (4) [below=.75cm of 0] {${e}'{ {} \seq {f}}$};
        \node (5) [left=0.5cm of 0] {{ \bcancel\checkmark}};
            \node (5a) [left=-.2cm of 5] {{ \(\partial({f})\)}};
        \draw (0) edge[-implies, double, double distance=0.5mm] node[above] {\({t}\)} (5);
        \draw (0) edge[-latex] node[left] {\footnotesize\( {a} \mid {s}\)} (4);
\end{tikzpicture}
\end{gather*}
Intuitively, $c_{f}$ reroutes the transitions coming out of ${e}$: acceptance (the first case) is replaced by the behaviour of ${f}$, and the probability mass of transitioning to ${e}'$ (the second case) is reassigned to $ e\seq f$.
A pictorial representation of the effect on the derivatives of ${e} \seq {f}$ is given above on the right.
Here, we assume that \(\partial({e})\) can perform a \({a}\)-transition to \({e'}\) with probability \({s}\); we make the same assumption in the informal descriptions of derivatives for the loops, below. 

For loops, we require $\partial\left(e^{[p]}\right)$ to be the least subdistribution satisfying $\partial\left(e^{[p]} \right) =  p \partial(e) \lhd e^{{[p]}} + (1-p) \partial(\checkmark)$. In the case when $\partial(e)(\checkmark)\neq 0$, the above becomes a fixpoint equation (as in such a case, the unrolling of the definition of $(- \lhd e^{[p]})$ involves $\partial \left( e{[p]}\right)$). We can define $\partial \left( e^{[p]} \right)$ as a closed form, but we need to consider two cases. If $\partial(e)(\checkmark)=1$ and $p = 1$, then the loop body is constantly executed, but the inner expression $e$ does not perform any labelled transitions. We identify such divergent loops with deadlock behaviour and hence $\partial(e^{[p]})(x)=0$. Otherwise, we look at $\partial(e)$ to build $\partial\left({e}^{[p]}\right)$. First, we make sure that the loop may be skipped with probability $1-p$.
Next, we modify the branches that perform labelled transitions by adding ${e}^{[p]}$ to be executed next.
The remaining mass is $p\partial(e)(\checkmark)$, the probability that we will enter the loop and immediately exit it without performing any labelled transitions. We discard this possibility and redistribute it among the remaining branches. 
As before, we provide an informal visual depiction of the probabilistic loop semantics below, using the same conventions as before. The crossed-out checkmark along with the dashed lines denotes the redistribution of probability mass described above.
\[
	\begin{tikzpicture}
        \node (0) {${e}^{[p]}$};
        \node (6) [below=.5cm of 0] {\checkmark};
        \node (4) [right=2cm of 0] {${e}'{ \seq {e}^{[{p}]}}$};
        \node (5) [left=.5cm of 0] {{ \(\bcancel{\checkmark}\)}};
        \draw (0) edge[-implies, double, double distance=0.5mm] node[left] {\footnotesize \(\frac{1-{p}}{1-{pt}}\)} (6);
        \draw (0) edge[-implies, double, double distance=0.5mm, pos=0.3] node[above, gray] {\({pt}\)} (5);
        \draw (0) edge[-latex] node[above] {\footnotesize\( {a} \mid {{{p}}}{s}/{ (1-{pt})}\)} (4);
        \draw (5) edge[->,dashed,bend left, out=90] ($(4.west) + (-0.5,0.35)$);
        \draw (5) edge[->,dashed,bend right, out=-90] ($(6.west) + (-0.5,0.35)$);
  %      \draw[dotted,bend right=20] (5) edge[-latex] (4);
   %     \draw[dotted,bend left=20] (5) edge[-latex] (6);
    \end{tikzpicture}
\]
Formally speaking, the definition of $\partial\left({e}^{[{p}]}\right)$ can be given by the following:
\begin{gather*}
\partial\left(e^{[p]}\right)(x)= \begin{cases}
    \frac{1-p}{1-p\partial(e)(\checkmark)} & x = \checkmark \\[1.2ex]
    \frac{p\partial(e)(a,e')}{1-p\partial(e)(\checkmark)} & x = (a, (e' \seq e^{[p]}))\\
    0 & \text{otherwise}
\end{cases}
\end{gather*}
% Having defined the semantics, we can easily observe that the termination operator $E : \PExp \to \interval{0}{1}$ captures the probability of immediate acceptance.
% \begin{restatable}{lemma}{exitoperatorlemma}\label{lem:exit_operator_lemma}
% For all $e \in \PExp$ it holds that $E(e)=\partial(e)(\checkmark)$
% \end{restatable}
% In our case, the canonical concept of coalgebraic bisimilarity (which in the case of {GPTS} directly corresponds to the notion from~\cite{Larsen:1991:Bisimulation}) is too discriminating notion of equivalence. For example, the states in the Brzozowski transition system for expressions $(a \oplus_{\frac{1}{2}} \zero) \seq (b \oplus_{\frac{1}{3}} \zero) $ and $(a \oplus_{\frac{1}{3}} \zero) \seq (b \oplus_{\frac{1}{2}} \zero) $ both generate a word $ab$ with probability $\frac{1}{6}$, while not being bisimilar. The state for the first expression can perform an $a$-transition with probability $\frac{1}{2}$, while the second one performs the same action with the probability of $\frac{1}{3}$, thus making them distinguished by the notion of bisimilarity.

% As mentioned before, if we omit the axioms marked with $\heartsuit$ from \cref{fig:axioms}, we obtain a coarser congruence relation ${\equiv_0} \subseteq \PExp \times \PExp$, sound wrt. coalgebraic bisimilarity. 
% % We will later use it in the proof of soundness of our axiomatisation wrt. language equivalence, as an intermediate step.
% \begin{restatable}{lemma}{soundnessbisim}\label{lem:soundness_bisim}
%     The relation ${\equiv_0} \subseteq {\PExp \times \PExp}$ is a bisimulation equivalence
% \end{restatable}
% As a consequence~\cite{Rutten:2000:Universal}, there exists a unique coalgebra structure $\ol{\partial} : {\PExp}/{\equiv_0} \to \distf F {\PExp}/{\equiv_0}$, which makes the quotient map $[-]_{\equiv_0} : \PExp \to {\PExp}/{\equiv_0}$ into $\distf F$-coalgebra homomorphism from $(\PExp, \equiv)$ to $({{\PExp}/{\equiv_0}}, \ol{\partial})$.

Having defined the Antimirov transition system, one can observe that the termination operator $E(-) \colon \PExp \to \interval{0}{1}$ precisely captures the probability of an expression transitioning to $\checkmark$ (successful termination) when viewed as a state in the Antimirov {GPTS}.

\begin{lemma}\label{c4:lem:exit_operator_lemma}
    For all $e \in \PExp$ it holds that $E(e)=\partial(e)(\checkmark)$.
\end{lemma}
Given an expression $e \in \PExp$, we write $\langle e\rangle \subseteq \PExp$ for the set of states reachable from $e$ by repeatedly applying $\partial$.
It turns out that the operational semantics of every {PRE} can be always described by a finite-state {GPTS} given by $(\langle e \rangle, \partial)$.
\begin{lemma}\label{lem:locally_finite}
    For all $e \in \PExp$, the set $\langle e \rangle$ is finite. In fact, the number of of states is bounded above by $\#(-) \colon \PExp \
    \to \N$, where $\#(-)$ is defined recursively by:
    \begin{align*}
        \#(\zero)=\#(\one)=1 \quad \#(a) = 2 \quad \#(e \oplus_p f) = \#(e) + \#(f)\\ 
        \#(e \seq f) = \#(e) + \#(f) \quad \#(e^{[p]}) = \#(e) + 1
    \end{align*}
\end{lemma}
\begin{proof}
    We adapt the analogous proof for {GKAT}~\cite{Schmid:2021:Guarded}.

    For any \({e} \in \PExp\), let \(|\langle e \rangle|\) be the cardinality of the carrier set of the least subcoalgebra of \((\PExp, \partial)\) containing \(e\). We show by induction that for all \(e \in \PExp\) it holds that \(|\langle e \rangle|\leq \#({e})\).

     For the base cases, observe that for $\zero$ and $\one$ the subcoalgebra has exactly one state. Hence, \(\#(\zero) = 1 = |\langle \zero \rangle|\). Similarly, we have \(\#(\one) = 1 = |\langle \one \rangle|\).
    For \(a \in \alphabet\), we have two states; the initial state, which transitions with probability \(1\) on \(a\) to the state which outputs \(\checkmark\) with probability \(1\).

    For the inductive cases, assume that \(|\langle e \rangle| \leq \#(e)\), \(|\langle f \rangle| \leq \#(f)\) and \(p \in \interval{0}{1}\).
     \begin{itemize}
         \item
         Every derivative of \({e} \oplus_p {f}\) is either a derivative of \({e}\) or \({f}\) and hence \(|\langle e \oplus_p f \rangle| \leq |\langle e \rangle| + |\langle f \rangle| = \#({e}) + \#({f}) = \#({e} \oplus_p {f})\).
         \item
         In the case of \({e}\seq{f}\), every derivative of this expression is either a derivative of \({f}\) or some derivative of \({e}\) followed by \({f}\). Hence, \(|\langle e \seq f \rangle| = |\langle e \rangle\times \{{f}\}| + |\langle f \rangle| \leq \#({e}) + \#({f}) = \#({e}\seq{f}) \).

         \item
         For the probabilistic loop case, observe that every derivative of \({e}^{[{p}]}\) is a derivative of \({e}\) followed by \({e}^{[{p}]}\) or it is the state that outputs $\checkmark$ with probability $1$. It can be easily observed,that \(|\langle e^{[p]}\rangle| \leq |\langle e \rangle| + 1 = \#({e}) = \#({e}^{[p]})\). 
    \end{itemize}
\end{proof}



\begin{example} Operational semantics of the expression $e = a \oplus_{\frac{3}{4}} a \seq a^{[\frac{1}{4}]}\seq a$ correspond to the following {GPTS}: 
\begin{center}
\begin{tikzpicture}%[baseline=0ex]
        \node (2) {$a \oplus_{\frac{3}{4}} a \seq a^{[\frac{1}{4}]}\seq a$};
        \node (2a) [ right= 2 cm of 2]{};
        \node (3) [below= .5 cm of 2a] {$a^{[\frac{1}{4}]}\seq a$};
        \draw (2) edge[-latex] node[below] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \draw (3) edge[-latex,out=-20,in=0,looseness=6] node[right] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \node (4) [right= 4.5 cm of 2] {$\one$};
        \draw (3) edge[-latex] node[below] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \draw (2) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \node (o2) [right= 0.75 cm of 4]{$\checkmark$};
        \draw (4) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{center}
One can observe that the transition system above for $e$ is isomorphic to the one starting in $q_2$ in \cref{ex:systems}.
\end{example}

Given the finite-state {GPTS} $(\langle e \rangle, \partial)$ associated with an expression $e\in \PExp$ we can define the language semantics of $e$ as the probabilistic language $\llbracket e\rrbracket \in \interval{0}{1}^{\alphabet^*}$ generated by the state $e$ in the {GPTS} $(\langle e \rangle, \partial)$. 

\subsection{Roadmap to soundness and completeness}\label{c4:subsec:roadmap}
The central aim of this chapter is to show that the axioms in \Cref{c4:fig:axioms} are sound and complete to reason about probabilistic language equivalence of {PRE}, that is:
\[
e \equiv f \quad \begin{array}{c} {\text{\tiny Completeness}} \\ \Longleftarrow\\ \Longrightarrow\\ {\text{\tiny Soundness}} \end{array} \llbracket e\rrbracket = \llbracket f\rrbracket
\]
We now sketch the roadmap on how we will prove these two results to ease the flow into the upcoming technical sections. Perhaps not surprisingly, the completeness direction is the most involved. 
%However, because of the rich algebraic structure of probabilistic languages, the soundness direction is not a simple straightforward inductive proof as in the case of regular expressions. 

The heart of both arguments will rely on arguing that the semantics map $\llbracket - \rrbracket \colon \PExp \to \interval{0}{1}^{\alphabet^\ast}$ assigning a probabilistic language to each expression can be seen as the following composition of maps:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXEV4cCJdLFsyLDAsIntcXEV4cH0ve1xcZXF1aXZ9Il0sWzQsMCwiWzAsMV1ee0FeKn0iXSxbMCwxLCJbLV0iXSxbMSwyLCJcXGRhZ2dlciBkIl0sWzAsMiwiXFxsbGJyYWNrZXQtXFxycmJyYWNrZXQiLDIseyJjdXJ2ZSI6M31dXQ==
\[\begin{tikzcd}
	\PExp && {{\PExp}/{\equiv}} && {\interval{0}{1}^{\alphabet^*}}
	\arrow["{[-]}", from=1-1, to=1-3]
	\arrow["{\beh{d}}", from=1-3, to=1-5]
	\arrow["{\llbracket-\rrbracket}"', curve={height=20pt}, from=1-1, to=1-5]
\end{tikzcd}\] 
In the picture above $[-] \colon \PExp \to {\PExp}/{\equiv}$ is a quotient map taking expressions to their equivalence class modulo the axioms of $\equiv$, while $\beh{d} \colon {\PExp}/{\equiv} \to \interval{0}{1}^{\alphabet^\ast}$ is a map taking each equivalence class to the corresponding probabilistic language. In such a case, soundness follows as a sequence of three steps:
\begin{equation}\label{eq:sound}
 e\equiv f\Rightarrow [e]=[f] \Rightarrow \dagger d ([e])  = \dagger d ( [f] ) \Rightarrow \llbracket e\rrbracket = \llbracket f\rrbracket 
 \end{equation}
 To obtain completeness we want to reverse all implications in \Cref{eq:sound}--and they all are easily reversible except $[e]=[f] \Rightarrow \beh{d} ( [e] )  =  \beh{d} ( [f] )$. To obtain this reverse implication we will need to show that $\beh{d} $ is {\em injective}.
 \section{Soundness}\label{c4:sec:soundness}
 \subsection{Step 1: Soundness with respect to bisimilarity}
 We first check that a subset of axioms generating $\equiv$ is sound with respect to bisimilarity of $\distf \funF$-coalgebras, which is a coarser notion of equivalence than probabilistic langauge equivalence. Let ${\equiv_b} \subseteq {\PExp \times \PExp}$ denote the least congruence relation closed under the axioms on \Cref{c4:fig:axioms} except (\textsf{S0}) and (\textsf{D2}). 

A straightforward induction on the length derivation of $\equiv_b$ allows us to show that this relation is a bisimulation equivalence on $(\PExp, \partial)$. As mentioned before, in the case of {GPTS} this notion corresponds to bisimulation equivalences in the sense of Larsen and Skou~\cite{Larsen:1991:Bisimulation}.
\begin{lemma}\label{c4:lem:soundness_bisim}
    The relation ${\equiv_b} \subseteq {\PExp \times \PExp}$ is a bisimulation equivalence.
 \end{lemma}
As a consequence of \Cref{c4:lem:soundness_bisim}~\cite{Rutten:2000:Universal}, there exists a unique coalgebra structure $\qcoa \colon {\PExp}/{\equiv_b} \to \distf \funF {\PExp}/{\equiv_b}$, which makes the quotient map $[-]_{\equiv_b} \colon \PExp \to {\PExp}/{\equiv_0}$ into a $\distf F$-coalgebra homomorphism from $(\PExp, \partial)$ to $({{\PExp}/{\equiv_b}},\qcoa)$. It turns out, that upon converting those $\distf \funF$-coalgebras to $\funG \distf$-coalgebras using the natural transformation $\rho \colon \distf \funF \to \funG \distf$ and determinising them, $\distf [-]_{\equiv_b} \colon \distf \PExp \to \distf {\PExp}/{\equiv_b}$ becomes a homomorphism between the determinisations. This situation can be summed up by the following commutative diagram:
%% https://q.uiver.app/#q=WzAsOCxbMCwxLCJcXFBFeHAiXSxbMCwyLCJcXGRpc3RmIFxcZnVuRiBcXFBFeHAiXSxbMCwzLCJcXGZ1bkdcXGRpc3RmIFxcUEV4cCJdLFsxLDAsIlxcZGlzdGYgXFxQRXhwIl0sWzIsMSwiXFxQRXhwL3tcXGVxdWl2X2J9Il0sWzMsMCwiXFxkaXN0ZiB7e1xcUEV4cH0ve1xcZXF1aXZfYn19Il0sWzIsMywiXFxmdW5HXFxkaXN0ZiB7XFxQRXhwfS97XFxlcXVpdl9ifSJdLFsyLDIsIlxcZGlzdGYgXFxmdW5GIHtcXFBFeHB9L3tcXGVxdWl2X2J9Il0sWzAsMSwiXFxwYXJ0aWFsIiwyXSxbMSwyLCJcXHJob197XFxQRXhwfSIsMl0sWzAsMywiXFxldGFfe1xcUEV4cH0iLDAseyJsYWJlbF9wb3NpdGlvbiI6NDB9XSxbNCw1LCJcXGV0YV97e1xcUEV4cH0ve1xcZXF1aXZfYn19IiwwLHsibGFiZWxfcG9zaXRpb24iOjEwfV0sWzMsNSwiXFxkaXN0ZiBbLV1fe1xcZXF1aXZfYn0iXSxbMCw0LCJbLV1fe1xcZXF1aXZfYn0iLDJdLFs0LDcsIlxccWNvYSIsMl0sWzcsNiwiXFxyaG9fe3tcXFBFeHB9L3tcXGVxdWl2X2J9fSIsMl0sWzIsNiwiXFxmdW5HIFxcZGlzdGYgWy1dX3tcXGVxdWl2X2J9Il0sWzMsMiwiKFxcZ2FtbWFfe1xcRXhwfSBcXGNpcmMgXFxwYXJ0aWFsKV5cXHNoYXJwIiwwLHsiY3VydmUiOi0zfV0sWzUsNiwiKFxcZ2FtbWFfe3tcXEV4cH0ve1xcZXF1aXZfMH19IFxcY2lyYyBcXHFjb2EpXlxcc2hhcnAiLDAseyJjdXJ2ZSI6LTR9XV0=
\[\begin{tikzcd}
	& {\distf \PExp} && {\distf {{\PExp}/{\equiv_b}}} \\
	\PExp && {\PExp/{\equiv_b}} \\
	{\distf \funF \PExp} && {\distf \funF {\PExp}/{\equiv_b}} \\
	{\funG\distf \PExp} && {\funG\distf {\PExp}/{\equiv_b}}
	\arrow["{\distf [-]_{\equiv_b}}", from=1-2, to=1-4]
	\arrow["{(\gamma_{\Exp} \circ \partial)^\sharp}", curve={height=-18pt}, from=1-2, to=4-1]
	\arrow["{(\gamma_{{\Exp}/{\equiv_0}} \circ \qcoa)^\sharp}", curve={height=-24pt}, from=1-4, to=4-3]
	\arrow["{\eta_{\PExp}}"{pos=0.4}, from=2-1, to=1-2]
	\arrow["{[-]_{\equiv_b}}"', from=2-1, to=2-3]
	\arrow["\partial"', from=2-1, to=3-1]
	\arrow["{\eta_{{\PExp}/{\equiv_b}}}"{pos=0.1}, from=2-3, to=1-4]
	\arrow["\qcoa"', from=2-3, to=3-3]
	\arrow["{\rho_{\PExp}}"', from=3-1, to=4-1]
	\arrow["{\rho_{{\PExp}/{\equiv_b}}}"', from=3-3, to=4-3]
	\arrow["{\funG \distf [-]_{\equiv_b}}", from=4-1, to=4-3]
\end{tikzcd}\]
 

 