\chapter{Completeness Theorem for Probabilistic Regular Expressions}
\label{chapter4}
\section{Overview}
In this section, we will introduce the syntax and the language semantics of probabilistic regular expressions ({PRE}), as well as a candidate inference system to reason about the equivalence of {PRE}. 

\subsection{Syntax}
Given a finite alphabet $\alphabet$, the syntax of {PRE} is given by:
$$e,f \in \PExp ::= \zero \mid \one \mid a \in A \mid e \oplus_p f \mid e\seq f \mid e^{[p]} \quad\quad\quad p \in\interval{0}{1}$$
We denote the expressions that immediately abort and successfully terminate by $\zero$ and $\one$ respectively. For every letter $a \in \alphabet$ in the alphabet, there is a corresponding expression representing an atomic action. Given two expressions $e, f \in {\PExp}$ and $p \in \interval{0}{1}$, probabilistic choice $e \oplus_p f$ denotes an expression that performs $e$ with probability $p$ and performs $f$ with probability $1-p$. One can think of $ \oplus_p$ as the probabilistic analogue of the
 plus operator ($e + f$) in Kleene's regular expressions. $e \seq f$ represents sequential composition, while $e^{[p]}$ is a probabilistic analogue of Kleene star: it successfully terminates with probability $1-p$ or with probability $p$ performs $e$ and then iterates $e^{[p]}$ again.  In terms of the notational convention, the sequential composition $(\seq)$ has higher precedence than the probabilistic choice $(\oplus_p)$.

\begin{example}
    The expression $a \seq a^{[\frac{1}{4}]}$ first performs action $a$ with probability $1$ and then enters a loop which successfully terminates with probability $\frac{3}{4}$ or performs action $a$ with probability $\frac{1}{4}$ and then repeats the loop again. Intuitively, if we think of the action $a$ as observable, the expression above denotes a probability associated with a non-empty sequence of $a$'s. For example, the sequence $aaa$ would be observed with probability $1 \cdot (1/4)^2 \cdot 3/4=3/64$.
\end{example}
\subsection{Language semantics}
{PRE} denote probabilistic languages $\alphabet^\ast \to \interval{0}{1}$. For instance, the expression $\zero$ denotes a function that assigns $0$ to every word, whereas $\one$ and $a$ respectively assign probability $1$ to the empty word and the word containing a single letter $a$ from the alphabet. The probabilistic choice $e \oplus_p f$ denotes a language in which the probability of each word is the total sum of its probability in $e$ scaled by $p$ and its probability in $f$ scaled by $1-p$. Describing the semantics of sequential composition and loops inductively is more involved. In particular, the semantics of loops would require a fixpoint calculation, which does not have as clear and straightforward (closed-form) formula, as the asterate of regular languages. Instead, we take an \emph{operational approach}, and we formally define the language semantics of {PRE} in \Cref{sec:language_semantics} through a small-step operational semantics, using a specific type of probabilistic transition system, which we introduce next.

\subsection{Generative probabilistic transition systems}
A {GPTS} consists of a set of states $Q$ and a transition function that maps each state $q\in Q$ to finitely many distinct outgoing arrows of the form:
\begin{itemize}
    \item \emph{successful termination} with probability $t$ (denoted $q \xRightarrow[]{t} \checkmark$), or 
    \item to another state $r$, via an \emph{$a$-labelled transition}, with probability $s \in [0,1] $ (denoted $q \xrightarrow[]{a \mid s} r$).
\end{itemize} 
We require that, for each state, the total sum of probabilities appearing on outgoing arrows sums up to less or equal to one. The remaining probability mass is used to model unsuccessful termination, hence the state with no outgoing arrows can be thought of as exposing deadlock behaviour. 

Given a word $w \in \alphabet^\ast$ the probability of it being generated by a state $q\in Q$ (denoted $\sem{ q}(w) \in \interval{0}{1}$) is defined inductively:
\begin{equation}\label{language}
    \llbracket q\rrbracket (\emptyword)=t \quad\text{if } q \xRightarrow[]{t} \checkmark \qquad\qquad
    \llbracket q \rrbracket(av) = \sum_{q \xrightarrow[]{a \mid s} r} s \cdot \llbracket r \rrbracket(v)
\end{equation}
We say that two states $q$ and $q'$ are \emph{language equivalent} if for all words $w \in A^*$, we have that $\llbracket q \rrbracket(w)=\llbracket q' \rrbracket(w)$.
\begin{example}\label{ex:systems}
 Consider the following {GPTS}:
 \begin{gather*}
\begin{tikzpicture}[baseline=-3ex]
        \node (0) {$q_0$};
        \node (1) [right= 1.15 cm of 0]{$q_1$};
        \draw (0) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {1}\)} (1);
        \draw (1) edge[loop above] node[left] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (1);
        \node (o1) [right= 0.5 cm of 1]{$\checkmark$};
        \draw (1) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( \frac{3}{4}\)} (o1);
        \node (2) [right= 0.5 cm of o1] {$q_2$};
         \node (2a) [right= 1.25 cm of 2] {};
        \node (3) [below= -.01 cm of 2a] {$q_3$};
        \draw (2) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \draw (3) edge[loop below] node[right] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \node (4) [right= 3 cm of 2] {$q_4$};
        \draw (3) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \draw (2) edge[-latex, bend left] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \node (o2) [right= 0.5 cm of 4]{$\checkmark$};
        \draw (4) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{gather*}
%States $q_0$ and $q_2$ despite not being bisimilar (in the sense from~\cite{Larsen:1991:Bisimulation}), 
States $q_0$ and $q_2$ both assign probability $0$ to the empty word $\emptyword$ and each word $a^{n+1}$ is mapped to the probability $\left(\frac{1}{4}\right)^n \cdot \frac{3}{4}$. Later in the paper, we show that the languages generated by states $q_0$ and $q_2$ can be specified using expressions $a\seq a^{\left[ \frac{1}{4} \right]}$ and $a \oplus_{\frac{3}{4}} ( a\seq a^{\left[ \frac{1}{4} \right]} \seq a)$ respectively.
\end{example}
In \Cref{sec:operational_semantics}, we will associate to each {PRE} $e$ an operational semantics or, more precisely, a state $q_e$ in a {GPTS}. The language semantics of $e$ will then be the language $\llbracket q_e \rrbracket \colon \alphabet^\ast \to \interval{0}{1}$ generated by $q_e$. Two {PRE} $e$ and $f$ are language equivalent if $\sem{q_e}=\sem{q_f}$. One of our main goals is to present a complete inference system to reason about language equivalence. In a nutshell, we want to present a system of (quasi-)equations of the form $e \equiv f$ such that:
\[
e\equiv f \Leftrightarrow \sem {q_e} = \sem{q_f}
\]
Such an inference system will have to contain rules to reason about all constructs of {PRE}, including probabilistic choice and loops. We describe next the system, with some intuition for the inclusion of each group of rules. 

\subsection{Axiomatisation of language equivalence of {PRE}} 
We define ${\equiv} \subseteq \PExp \times \PExp$ to be the least congruence relation closed under the axioms shown on \cref{fig:axioms}. We will show in \Cref{sec:completeness} that these axioms are complete with respect to language semantics. 

%\begin{figure*}
%        \centering
%        \begin{multicols}{2}
%        	 \begin{flushleft}\underline{\bf Probabilistic Choice}  	 \end{flushleft}
%        \begin{alignat*}{4}
%		\textbf{(C1)} & \;\, 
%		& e &{} \equiv e \oplus_p e\\
%				\textbf{(C2)} & \;\, 
%		& e &{} \equiv e \oplus_1 f\\
%		\textbf{(C3)} & \;\, 
%		& e \oplus_p  f &{} \equiv f \oplus_{\overline{p}} e\\
%		\textbf{(C4)} & \;\, 
%		& (e \oplus_{p} f) \oplus_{q} g & {} \equiv e \oplus_{pq} (f \oplus_{\frac{{(1-p)}q}{1-pq}} g)\\
%		\textbf{(D1)} & \;\, 
%		& (e \oplus_{p} f) \seq g & {} \equiv e \seq g \oplus_{p} f \seq g\\[-0.25ex]
%		\textbf{(D2)} & \;\, 
%		& e \seq (f \oplus_{p} g) & {} \equiv e \seq g \oplus_{p} e \seq f\\[-0.25ex]\\\\
%        \end{alignat*}
%          \begin{flushleft}   \underline{\bf Sequencing} \end{flushleft}
%        \begin{alignat*}{4}
%        \textbf{(0S)} & \;\, 
%		& 0 \seq e & {} \equiv 0 & \\[-0.25ex]
%		\textbf{(S0)} & \;\, 
%		& e \seq 0 & {} \equiv 0 & \\[-0.25ex]
%		\textbf{(1S)} & \;\, 
%		& 1 \seq e & {} \equiv e & \\[-0.25ex]
%		\textbf{(S1)} & \;\, 
%		& e \seq 1 & {} \equiv e \\[-0.25ex]
%		\textbf{(S)} & \;\, 
%		& e \seq (f \seq g) & {} \equiv (e \seq f ) \seq g \\[-0.25ex]
%        \end{alignat*}
%        \end{multicols}
%        \vspace{-2cm}
%         \begin{multicols}{2}
%     \begin{flushleft}    \underline{\bf Loops} \end{flushleft}
%        \begin{alignat*}{4}
%       	\textbf{(Unroll)} & \;\, 
%		& e \seq e^{[p]} \oplus_p 1& {} \equiv e^{[p]} \\
%		\textbf{(Tight)} & \;\, 
%		& (e \oplus_p 1)^{[q]} \seq 1 & {} \equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]} \\[-0.25ex]
%		\textbf{(Div)} & \;\, 
%		& 1^{[1]} & {} \equiv 0 \\
%%		 \\ \quad\;\;\textbf{(Unique)} \;\, 
%%		\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}
%        \end{alignat*}
%          \begin{flushleft}   \underline{\bf (Unique) fixpoint rule} \end{flushleft}
%		$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$
%\end{multicols}
%        \vspace{-.5cm}
%     \begin{flushleft}    \underline{\bf Termination cond. $E : \PExp \to [0,1]$} \end{flushleft}
%        \fcolorbox{red}{white}{$
%        \begin{array}{l}
%            \quad E(\one)=1 \qquad E(\zero)=E(a) = 0 \qquad
%            E \left( e \oplus_p f \right)=pE(e) + \ol{p}E(f)\qquad 
%            E(e\seq f)=E(e)E(f)\quad \\[1.4ex]
%            \quad E\left(e^{[p]}\right)=
%                \begin{cases}
%                0 &  \text{\scriptsize $E(e)=1 \wedge p=1$} \\ 
%                \frac{1-p}{1-pE(e)} & \text{\scriptsize otherwise}
%                \end{cases}   
%                \end{array}$
%     }
%     %        \end{tabular}
%      %		\begin{align*}
%%            \textbf{(Unroll)} \quad e^{[p]} &\equiv e \seq e^{[p]}\\
%%            \textbf{(Tight)} \quad (e \oplus_p \one)^{[q]} &\equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]}\\
%%            \one^{[1]} &\equiv \zero &\textbf{(Div)}\\
%%           \omit\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}&&\textbf{(Unique)}\\
%%        \end{align*}
%        \caption{\label{fig:axioms} For $p \in [0,1]$, we write $\ol{p}=1-p$. The rules involving the division of probabilities are defined only when the denominator is non-zero. The function $E(-)$ provides a termination side condition to the \textbf{Unique} fixpoint axiom.}
%            \vspace{-.4cm}
%    \end{figure*}
%We will also write ${\equiv_0} \subseteq {\PExp \times \PExp}$ for the least congruence relation containing all the above rules except the two marked with $\heartsuit$. Intuitively, removing these rules axiomatises bisimilarity, which is the finer notion of equivalence. This relation will be used in the intermediate step of the proof of soundness. We will use the following notation for the quotient maps:
%% https://q.uiver.app/?q=WzAsMyxbMSwwLCJ7XFxFeHB9L3tcXGVxdWl2XzB9Il0sWzIsMCwie1xcRXhwfS97XFxlcXVpdn0iXSxbMCwwLCJcXEV4cCJdLFswLDEsIlstXV97XFxlcXVpdn0iLDJdLFsyLDAsIlstXV97XFxlcXVpdl8wfSIsMl0sWzIsMSwiWy1dIiwwLHsiY3VydmUiOi0zfV1d
%\[\begin{tikzcd}
%	\PExp & {{\PExp}/{\equiv_0}} & {{\PExp}/{\equiv}}
%	\arrow["{[-]_{\equiv}}"', from=1-2, to=1-3]
%	\arrow["{[-]_{\equiv_0}}"', from=1-1, to=1-2]
%	\arrow["{[-]}", curve={height=-18pt}, from=1-1, to=1-3]
%\end{tikzcd}\]

The first group of axioms capture properties of the probabilistic choice operator $\oplus_p$ (\textbf{C1-C4}) and its interaction with sequential composition (\textbf{D1-D2}). Intuitively, \textbf{C1-C4} are the analogue of the semilattice axioms governing the behaviour of $+$ in regular expressions. These four axioms are reminiscent of the axioms of barycentric algebras~\cite{Stone:1949:Postulates}. \textbf{D1} and \textbf{D2} are \emph{right and left distributivity} rules of $\oplus$ over $\seq$. The sequencing axioms \textbf{1S, S1, S}  state {PRE} have the structure of a monoid (with neutral element $\one$) with absorbent element $\zero$ (\textbf{0S, S0}). The loop axioms contain respectively \emph{unrolling}, \emph{tightening}, and \emph{divergency} axioms plus a \emph{unique fixpoint} rule. The \textbf{Unroll} axiom associates loops with their intuitive behaviour of choosing, at each step, probabilistically between successful termination and executing the loop body once. \textbf{Tight} and \textbf{Div} are the probabilistic analogues of the identity $(e + \one)^{*} \equiv e^{*}$ from regular expressions. In the case of {PRE}, we need two axioms: \textbf{Tight} states that the probabilistic loop whose body might instantly terminate, causing the next loop iteration to be executed immediately is provably equivalent to a different loop, whose body does not contain immediate termination; \textbf{Div} takes care of the edge case of a no-exit loop and identifies it with failure. Finally, the unique fixpoint rule is a re-adaptation of the analogous axiom from Salomaa's axiomatisation and provides a partial converse to the loop unrolling axiom, given the loop body is productive -- i.e. cannot immediately terminate. This productivity property is formally written using the side condition $E(e) = 0$, which can be thought of as the probabilistic analogue of empty word property from Salomaa’s axiomatisation. Consider an expression $a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)$. The only way it can accept the empty word is to leave the loop with the probability of $\frac{1}{2}$ and then perform $1$, which also can happen with probability $\frac{1}{2}$. In other words, $\llbracket a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)\rrbracket(\emptyword) = \frac{1}{4}$. A simple calculation allows to verify that $E( a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} \one)) = \frac{1}{4}$.




\begin{example} We revisit the expressions from \cref{ex:systems} and show their equivalence via axiomatic reasoning.
\begin{align*}
     a \seq a^{[\frac{1}{4}]} &\stackrel\dagger\equiv   a \seq (a^{[\frac{1}{4}]} \seq a \oplus_{\frac{1}{4}} \one) \stackrel{\textbf{D2}}\equiv  (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \seq \one\\  &\stackrel{\textbf{S1}}\equiv (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \stackrel{\textbf{C3}}\equiv a \oplus_{\frac{3}{4}} (a \seq a^{[\frac{1}{4}]}\seq a)
\end{align*}

The $\dagger$ step of the proof above relies on the equivalence $e^{[p]}\seq e \oplus_p \one \equiv e$ derivable from other axioms under the assumption $E(e)=0$ through a following line of reasoning:
\begin{align*}
    e^{[p]} \seq e \oplus_p \one  &\equiv (e \seq e^{[p]} \oplus_p \one)\seq e\oplus_p \one \tag{\textbf{Unroll}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p \one\seq e) \oplus_p \one \tag{\textbf{D1}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p e) \oplus_p \one \tag{\textbf{1S}}\\
        &\equiv (e \seq e^{[p]}\seq e \oplus_p e\seq \one) \oplus_p \one \tag{\textbf{S1}}\\
    &\equiv e \seq (e^{[p]}\seq e \oplus_p \one) \oplus_p \one \tag{\textbf{D2}}
\end{align*}
Since $E(e)=0$,  we then have: 
\(
     e^{[p]} \seq e \oplus_p \one \stackrel{(\textbf{Unique})}\equiv e^{[p]}\seq \one 
     \stackrel{(\textbf{S1})}\equiv e^{[p]} .
\)
\end{example}
\subsection{High-level overview}
The central aim of this chapter is to show that the axioms in \Cref{c4:fig:axioms} are sound and complete to reason about probabilistic language equivalence of {PRE}, that is:
\[
e \equiv f \quad \begin{array}{c} {\text{\tiny Completeness}} \\ \Longleftarrow\\ \Longrightarrow\\ {\text{\tiny Soundness}} \end{array} \llbracket e\rrbracket = \llbracket f\rrbracket
\]
We now sketch the roadmap on how we will prove these two results to ease the flow into the upcoming technical sections. Perhaps not surprisingly, the completeness direction is the most involved. 
%However, because of the rich algebraic structure of probabilistic languages, the soundness direction is not a simple straightforward inductive proof as in the case of regular expressions. 

The heart of both arguments will rely on arguing that the semantics map $\llbracket - \rrbracket \colon \PExp \to [0,1]^{\alphabet^\ast}$ assigning a probabilistic language to each expression can be seen as the following composition of maps:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXEV4cCJdLFsyLDAsIntcXEV4cH0ve1xcZXF1aXZ9Il0sWzQsMCwiWzAsMV1ee0FeKn0iXSxbMCwxLCJbLV0iXSxbMSwyLCJcXGRhZ2dlciBkIl0sWzAsMiwiXFxsbGJyYWNrZXQtXFxycmJyYWNrZXQiLDIseyJjdXJ2ZSI6M31dXQ==
\[\begin{tikzcd}
	\PExp && {{\PExp}/{\equiv}} && {[0,1]^{A^*}}
	\arrow["{[-]}", from=1-1, to=1-3]
	\arrow["{\beh{d}}", from=1-3, to=1-5]
	\arrow["{\llbracket-\rrbracket}"', curve={height=20pt}, from=1-1, to=1-5]
\end{tikzcd}\] 
In the picture above $[-] \colon \PExp \to {\PExp}/{\equiv}$ is a quotient map taking expressions to their equivalence class modulo the axioms of $\equiv$, while $\beh{d} \colon {\PExp}/{\equiv} \to \interval{0}{1}^{\alphabet^\ast}$ is a map taking each equivalence class to the corresponding probabilistic language. In such a case, soundness follows as a sequence of three steps:
\begin{equation}\label{eq:sound}
 e\equiv f\Rightarrow [e]=[f] \Rightarrow \dagger d ([e])  = \dagger d ( [f] ) \Rightarrow \llbracket e\rrbracket = \llbracket f\rrbracket 
 \end{equation}
 To obtain completeness we want to reverse all implications in \Cref{eq:sound}--and they all are easily reversible except $[e]=[f] \Rightarrow \beh{d} ( [e] )  =  \beh{d} ( [f] )$. To obtain this reverse implication we will need to show that $\beh{d} $ is {\em injective}.
{\color{red} Here I need more text that will explain the tools we gonna use	}
\section{Preliminaries}
\label{c4:sec:preliminaries}
\subsection{Locally finite categories}
$\D$ is a filtered category, if every finite subcategory $\D_0 \hookrightarrow \D$ has a cocone in $\D$. A filtered colimit is a colimit of the diagram $\D \to \C$, where $\D$ is a filtered category. A directed colimit is a colimit of the diagram $\D \to \C$, where $\D$ is a directed poset. We call a functor \emph{finitary} if it preserves filtered colimits. An object $C$ is \emph{finitely presentable (fp)} if the representable functor $\C(C,-) : \C \to \Set$ preserves filtered colimits. Similarly, an object $C$ is \emph{finitely generated (fg)} if the representable functor $\C(C,-) : \C \to \Set$ preserves directed colimits of monos. Every fp object is fg, but the converse does not hold in general. A category $\C$ is \emph{locally finitely presentable (lfp)} if it is cocomplete and there exists a set of finitely presentable objects, such that every object of $\C$ is a filtered colimit of objects from that set. $\Set$ is the prototypical example of a locally finitely presentable category, where finite sets are the fp objects.

\subsection{Monads and their algebras}\label{c4:subsec:monads}
A monad (over the category $\Set$) is a triple $\monadT = (T, \mu, \eta)$ consisting of a functor $T \colon \Set \to \Set$ and two natural transformations: a unit $\eta \colon \Id \Rightarrow T$ and multiplication $\mu \colon T^2 \Rightarrow T$ satisfying $\mu \circ \eta_T = \id_T=  \mu \circ T\eta$ and $\mu \circ \mu_T = \mu \circ T \mu$
A $\monadT$-algebra (also called an Eilenberg-Moore algebra) for a monad $T$ is a pair $(X, h)$ consisting of a set $X \in \Obj(\C)$, called carrier, and a function $h \colon T X \to X$ such that $h \circ \mu_X = h \circ Th$ and $h \circ \eta_X = \id_X$. A $\monadT$-algebra homomorphism between two $T$-algebras $(X,h)$ and $(Y,k)$ is a function 	$f \colon X \to Y$ satisfying $k \circ Tf = f \circ h$.

$\monadT$-algebras and $\monadT$-homomorphisms form a category $\Set^\monadT$. There is a canonical forgetful functor $\forget \colon \Set^\monadT \to \Set$ that takes each $\monadT$-algebra to its carrier. This functor has a left adjoint $X \mapsto (TX, \mu_X \colon T^2 X \to T)$, mapping each set to its free $\monadT$-algebra. If $X$ is finite, then we call $(TX, \mu_X)$ free finitely generated.

Given a function $f \colon X \to Y$, where $Y$ is a carrier of a $\monadT$-algebra $(Y, h)$, there is a unique homomorphism $f^\sharp \colon (TX, \mu_X) \to (Y,h)$ satisfying $f^\sharp \circ \eta_X = f$ that is explicitly given by $f^\sharp = h \circ Tf$.

\subsection{Generalised determinisation}\label{sec:c4:generalised_determinisation}
Language acceptance of nondeterministic automata (NDA) can be captured via determinisation. {NDA} can be viewed as coalgebras for the functor $N = 2 \times {\powf}^\alphabet$, where $\powf$ is the finite powerset monad. Determinisation converts a {NDA} $(X, \beta \colon X \to 2 \times {\powf X}^\alphabet)$ into a deterministic automaton $\left({\powf X}, \beta^{\sharp} \colon {\powf X} \to 2 \times {(\powf X)}^\alphabet \right)$, where for $A \subseteq X$, we define $\beta^{\sharp}(A) = \bigcup_{x \in A} \beta(a)$. Additionally, $\beta^\sharp$ satisfies $\beta^\sharp(\{x\}) = \beta(x)$ for all $x \in X$. A language of the state $x \in X$ of {NDA}, is given by the language accepted by the state $\{x\}$ in the determinised automaton $({\powf X}, \beta^\sharp)$. 


This construction can be generalised~\cite{Silva:2010:Generalizing} to $HT$-coalgebras, where $T \colon \Set \to \Set$ is an underlying functor of finitary monad $\monadT$ and $H \colon \Set \to \Set$ an endofunctor that admits a final coalgebra that can be lifted to the functor $\overline{H} \colon \Set^\monadT \to \Set^\monadT$. Liftings of functors $H \colon \Set \to \Set$ to $\overline{H} \colon \Set^\monadT \to \Set^\monadT$, are in one-to-one correspondence with distributive laws of the monad $\mathsf{T}$ over the functor $H$~\cite{Jacobs:2015:Trace}, which are natural transformations $\lambda \colon TH \Rightarrow HT$ satisfying
$H\eta_X = \lambda_X \circ \eta_{HX}$ and $H \mu_X \circ \lambda_{TX} \circ T\lambda_X = \lambda_X \circ \mu_{HX}$. 

Generalised determinisation turns $HT$-coalgebras $(X, \beta \colon X \to HT X)$ into $H$-coalgebras $(T X , \beta^\sharp \colon T X \to H T X)$, where $\beta^\sharp$ is the unique extension arising from the free-forgetful adjunction between $\Set$ and $\Set^\monadT$. The language of a state $x \in X$ is given by $\beh{\beta^\sharp} \circ \eta_X \colon X \to \nu H$, where $\eta$ is the unit of the monad $T$. Since $\beta^\sharp \colon TX \to HTX$ can be seen as a $\monadT$-algebra homomorphism $(TX, \mu_X) \to \overline{H}(TX, \mu_X)$, each determinisation $(TX, \beta^{\sharp})$ can be viewed as an $\overline{H}$-coalgebra $((TX, \mu_X), \beta^{\sharp})$. The carrier of the final $H$-coalgebra can be canonically equipped with $\monadT$-algebra structure, yielding the final $\overline{H}$-coalgebra. In such a case, the unique final homomorphism from any determinisation (viewed as an $H$-coalgebra) is precisely an underlying function of the final $\overline{H}$-coalgebra homomorphism. 

\subsection{Subdistribution monad}\label{c4:subsec:subdistribution}
 A function $\nu \colon X \to [0,1]$ is called a subprobability distribution or subdistribution, if it satisfies $\sum_{x \in X} \nu(x) \leq 1$. A subdistribution $\nu$ is \emph{finitely supported}  if the set $\supp(\nu) = \{x \in X \mid \nu(x) > 0\}$ is finite. We use $\distf {X}$ to denote the set of finitely supported subprobability distributions on $X$. The weight of a subdistribution $\nu \colon X \to \interval{0}{1}$ is a total probability of its support:
$$|\nu| = \sum_{x \in X} \nu(x)$$
Given $\nu\in\distf X$ and $Y \subseteq X$, we will write $\nu[Y] = \sum_{x \in Y} \nu(x)$. This sum is well-defined as only finitely many summands have non-zero probability. 
 
  
 Given $x \in X$, its \emph{Dirac} is a subdistribution $\delta_x$ which is given by $\delta_x(y)=1$ only if $x=y$, and $0$ otherwise. We will moreover write $\emptydist \in \distf X$ for a subdistribution with an empty support. It is defined as $\emptydist(x)=0$ for all $x \in X$. When $\nu_1, \nu_2 \colon X \to \interval{0}{1}$ are subprobability distributions and $p \in \interval{0}{1}$, we write $p\nu_1 + (1-p)\nu_2$ for the convex combination of $\nu_1$ and $\nu_2$, which is the probability distribution given by $$(p \nu_1 + (1-p) \nu_2)(x) = p\nu_1(x) + (1-p)\nu_2(x)$$
 for all $x \in X$. Note that this operation preserves finite support. 
 
  $\distf$ is in fact a functor on the category $\Set$, which maps each set $X$ to $\distf X$ and maps each arrow $f \colon X \to Y$ to the function $\distf f \colon \distf X \to \distf Y$ given by $$\distf f (\nu)(x) = \sum_{y \in f^{-1}(x)} \nu(y)$$
   Moreover, $\distf$ also  carries a monad structure with unit  $\eta_X(x) = \delta_x$ and multiplication $\mu_X(\Phi)(x) = \sum_{\varphi \in \distf X }\Phi(\varphi)\varphi(x)$ for $\Phi \in \distf^2 X$. Using the free-forgetful adjunction between $\Set$ and category of $\distf$-algebras, given $f \colon X \to \distf Y$, there exists a unique map $f^\sharp \colon \distf X \to \distf Y$ satisfying $f = f^\sharp \circ \delta$ called the \emph{convex extension of $f$}, and explicitly given by $f^\sharp(\nu)(y) = \sum_{x \in X} \nu(x) f(x)(y)$.
   
  Below, we recall the notions surrounding couplings of (sub)probability distributions, which will be used in one of the intermediate steps of the proof of soundness.
\begin{definition}[{\cite[Definition~2.1.2]{Hsu:2017:Probabilistic}}]\label{def:coupling}
    Given two subdistributions $\nu_1, \nu_2$ over $X$ and $Y$ respectively, a subdistribution $\nu$ over $X \times Y$ is called coupling if:
    \begin{enumerate}
        \item For all $x \in X$, $\nu_1(x) = \nu[\{x\}\times Y]$
        \item For all $y \in Y$, $\nu_2(y)= \nu[X \times \{y\}]$
    \end{enumerate} 
\end{definition}
It can be straightforwardly observed that a coupling $\nu$ of $(\nu_1, \nu_2)$ is finitely supported if and only if both $\nu_1$ and $\nu_2$ are finitely supported.
\begin{definition}[{\cite[Definition~2.1.7]{Hsu:2017:Probabilistic}}]\label{def:lifting}
    Let $\nu_1, \nu_2$ be subdistributions over $X$ and $Y$ respectively and let $R \subseteq X \times Y$ be a relation. A subdistribution $\nu$ over $X \times Y$ is a \emph{witness} for the $R$-\emph{lifting} of $(\nu_1, \nu_2)$ if:
    \begin{enumerate}
        \item $\nu$ is a coupling for $(\nu_1, \nu_2)$
        \item $\supp(\mu) \subseteq R$
    \end{enumerate}
\end{definition}
If there exists $\nu$ satisfying these two conditions, we say $\nu_1$ and $\nu_2$ are related by the \emph{lifting} of $R$ and write $\nu_1 \equiv_R \nu_2$. It can be immediately observed that $\nu_1 \equiv_R \nu_2$ implies $\nu_2 \equiv_{R^{-1}} \nu_1$.

Given a relation ${R}\subseteq{X \times Y}$ and a set $B \subseteq X$, we will write $R(B) \subseteq Y$ for the set given by $R(B)= \{y \in Y \mid (x,y) \in R \text{ and } x \in B\}$. We will write $R^{-1} \subseteq Y \times X$ for the converse relation given by $R^{-1} = \{(y,x) \in Y \times X \mid (x,y) \in R\}$. 
\begin{theorem}[{\cite[Theorem 2.1.11]{Hsu:2017:Probabilistic}}]\label{thm:coupling_theorem}
Let $\nu_1$, $\nu_2$ be subdistributions over $X$ and $Y$ respectively and let $R \subseteq X \times Y$ be a relation. Then the lifting $\nu_1 \equiv_R \nu_2$ implies $\nu_1[B] \leq \nu_2[R(B)]$ for every subset $B \subseteq X$. The converse holds if $\mu_1$ and $\mu_2$ have equal weight.
\end{theorem}
\begin{lemma}\label{lem:coupling_lemma}
Let $\nu_1$, $\nu_2$ be subdistributions over $X$ and $Y$ respectively and let $R \subseteq X \times Y$ be a relation. $\nu_1 \equiv_R \nu_2$ if and only if:
\begin{enumerate}
	\item For all $B \subseteq X$,  $\nu_1[B] \leq \nu_2[R(B)]$
	\item For all $C \subseteq Y$, $\nu_2[C] \leq \nu_1[R^{-1}(C)]$
\end{enumerate}
\end{lemma}
\begin{proof}
	Assume that $\nu_1 \equiv_R \nu_2$. Recall, that in such a case $\nu_2 \equiv_{R^{-1}} \nu_2$. Applying \cref{thm:coupling_theorem} yields (1) and (2) respectively.
	
	For the converse, assume (1) and (2) do hold. We have the following:
\begin{align*}
	|\nu_1| &= \nu_1[X] \\
	&\leq \nu_2[R(X)] \tag{1} \\
	&\leq \nu_2[Y] \tag{$R(X) \subseteq Y$}\\
	&= |\nu_2|
\end{align*}
By a symmetric reasoning involving (2), we can show that $|\nu_2| \leq |\nu_1|$ and therefore $|\nu_1| = |\nu_2|$. Since condition (1) holds, we can use \Cref{thm:coupling_theorem} to conclude that $\nu_1 \equiv_R \nu_2$.
\end{proof}
  
\subsection{Positive convex algebras}\label{c4:subsec:positive}
By $\sigpca$ we denote a signature given by
$$\sigpca = \left\{\bigboxplus_{i \in I} p_i \cdot (-)_i \mid I \text{ finite}, \forall i \in I \ldotp p_i \in [0,1], \sum_{i \in I} p_i \leq 1\right\} $$
A positive convex algebra is a an algebra for the signature $\sigpca$, that is a pair $\A = \left(X, \sigpca^\A\right)$, where $X$ is the carrier set and $\sigpca^\A$ is a set of interpretation functions $\bigboxplus_{i \in I} p_i \cdot (-)_i \colon X^{|I|} \to X$ satisfying the axioms:
\begin{enumerate}
    \item (Projection) \(\bigboxplus_{i \in I} p_i \cdot x_i = x_j\) if \(p_j=1\)
    \item (Barycenter) \(\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J}{q_{i,j}} \cdot {x_j}\right) = \bigboxplus_{j \in J} \left(\sum_{i \in I} p_i q_{i,j} \right) \cdot x_j\)
\end{enumerate}
In terms of notation, we denote the unary sum by $p_0 \cdot x_0$. Throughout this chapter we will we abuse the notation by writing $$\left(\bigboxplus_{i \in I} p_i \cdot e_i\right) \boxplus \left(\bigboxplus_{i \in J} q_j \cdot f_j\right)$$ for a single sum $\bigboxplus_{k \in I + J} r_k \cdot g_k$, where $r_k = p_k$ and $g_k = e_k$ for $k \in I$ and similarly $r_k = q_k$ and $g_k = f_k$ for $k \in J$. Note that this is well-defined only if $\sum_{i \in I} p_i + \sum_{j \in J} r_j \leq 1$.

The signature of positive convex algebras can be alternatively presented as a family of binary operations, in the following way:

\begin{proposition}\label{c4:prop:binary}
    If  $X$ is a set equipped with a binary operation $\boxplus_{p} : X \times X \to X$ for each $p \in [0,1]$ and a constant $0_\boxplus \in X$ satisfying for all $x,y,z \in X$ (when defined) the following:
    \begin{gather*}
        x \boxplus_{p} x = x \qquad x \boxplus_{1} y = x \qquad x \boxplus_{p} y = y \boxplus_{1-{p}} x \\ (x \boxplus_{p} y) \boxplus_{q} z = x \boxplus_{pq} \left(y \boxplus_{\frac{(1-p)q}{1-pq}} z\right)
    \end{gather*}
    then $X$ carries the structure of a positive convex algebra. The interpretation of $\boxplus_{i \in I} p_i \cdot (-)_{i}$ is defined inductively by the following
    $$\bigboxplus_{i \in I} p_i \cdot x_i = \begin{cases}
        0_\boxplus & \text{if } I = \emptyset \\
        x_0 & \text{if } p_0 = 1\\
        x_n \boxplus_{p_k} \left(\bigboxplus_{i \in I \setminus \{k\}} \frac{p_i}{1-{p_k}}\cdot x_i  \right) &\text{otherwise, for some } k \in I
    \end{cases} $$
\end{proposition}
%\begin{proof}
%    A straightforward re-adaptation of \cite[Proposition~7]{Bonchi:2017:Power}.
%\end{proof}
Below we state several properties of positive convex algebras, that we will use throughout this chapter.
\begin{proposition}\label{c4:prop:properties_of_positive_convex_algebras}
    Let $I$ be a finite indexed set, and let $\{p_i\}_{i \in I}$ and $\{x_i\}_{i \in I}$ be indexed collections of elements of $\interval{0}{1}$ and $X$ respectively. Then, in any positive convex algebra, the following statements hold:
    \begin{enumerate}
        \item
        $${\bigboxplus_{i \in I} p_i \cdot x_i} = {\bigboxplus_{x \in \bigcup_{i \in I} \{x_i\}} \left(\sum_{x_i = x} p_i\right)\cdot x}$$
        \item Let ${=_R} \subseteq {X \times X}$ be a congruence relation, with $[-]_R \colon X \to X/{=_R}$ being its canonical quotient map. Then, 
        $${\bigboxplus_{i \in I} p_i \cdot x_i} =_R {\bigboxplus_{[x]_R \in \bigcup_{i \in I} \left\{[x_i]_R\right\}} \left(\sum_{x_i =_R x} p_i\right)\cdot x}$$
        \item All terms $\bigboxplus_{i \in I } 0 \cdot x_i $ coincide and are all provably equivalent to the empty convex sum.
        \item If $J \subseteq I$ and $J \supseteq \{i \in I \mid p_i \neq 0\}$, then 
        $$
        \bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{j \in J} p_j \cdot x_j
        $$
        \item Let $\sigma \colon I \to I$ be a permutation of the index set $I$. Then, we have that
        $$
        	\bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{i \in I} p_{\sigma(i)} \cdot x_{\sigma(i)}
        $$
    \end{enumerate}
\end{proposition}
\begin{proof}
    We write $[\Phi]$ to denote Iverson bracket, which is defined to be $1$ if $\Phi$ is true and $0$ otherwise.
    
    For \circlednum{1} we have that
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot x_i &= \bigboxplus_{i \in I} p_i \cdot \left( \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} [x_i = x] \cdot  x \right) &\tag{Projection axiom}\\
        &= \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} \left( \sum_{i \in I} p_i[x_i = x] \right) \cdot x \tag{Barycenter axiom} \\
        &=  \bigboxplus_{x \in \cup_{i \in I} \{x_i\} } \left(\sum_{x_i = x} p_i\right) \cdot x
    \end{align*}
    \circlednum{2} can be shown by picking a representative for each equivalence class and then using \circlednum{1}. For \circlednum{3}, by \cite[Lemma~3.4]{Sokolova:2015:Congruences} we know that all terms $\bigboxplus_{i \in I} 0 \cdot x_i$ coincide. To see that they are provably equivalent to the empty convex sum, observe that
    \begin{align*}
        \bigboxplus_{i \in I} 0 \cdot x_i &= \bigboxplus_{i \in I} 0 \cdot \left(\bigboxplus_{j \in \emptyset} p_j \cdot y_j\right)\tag{\cite[Lemma~3.4]{Sokolova:2015:Congruences}}\\
        &= \bigboxplus_{j \in \emptyset} 0 \cdot y_j \tag{Barycenter axiom}\\
    \end{align*}
    Finally, \circlednum{4} follows from \cite[Lemma~3.4]{Sokolova:2015:Congruences}, while \circlednum{5} was proved in \cite[Proposition~3.1]{Doberkat:2008:Erratum}.
\end{proof}
\begin{lemma}\label{lem:flattening_convex_sums}
    Let $I, J$ be finite index sets, $\left\{p_i\right\}_{i \in I}$, $\{q_{i,j}\}_{(i,j) \in I\times J}$ and $\{x_{i,j}\}_{(i,j) \in I \times J}$ indexed collections such that for all $i \in I$ and $j \in J$, $p_i, q_{i,j} \in [0,1]$ and $x_{i,j} \in X$. If $X$ carries $\pca$ structure, then:
    $$\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) = \bigboxplus_{(i,j) \in I \times J} p_iq_{i,j} \cdot x_{i,j}$$
\end{lemma}
\begin{proof}
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in \{i\} \times J} q_{k,j} \cdot x_{k,j}\right) \\
        &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in I \times J} [k = i]q_{k,j} \cdot x_{k,j}\right) \tag{\Cref{c4:prop:properties_of_positive_convex_algebras}} \\
        &= \bigboxplus_{(k,j) \in I \times J} \left(\sum_{i \in I} p_i [k=i]q_{k,j} \right) \cdot x_{k,j} \tag{Barycenter axiom} \\
        &= \bigboxplus_{(k,j) \in I \times J} p_k q_{k,j}  \cdot x_{k,j} \\
        &= \bigboxplus_{(i,j) \in I \times J} p_i q_{i,j}  \cdot x_{i,j} \tag{Renaming indices}\\
    \end{align*}
\end{proof}
\begin{lemma}\label{c4:lem:grouping_probabilities}
    Let $I$ be a finite index set, $\{p_i\}_{i \in I}$ and $\{q_i\}_{i \in I}$ indexed collections such that $p_i,q_i \in [0,1]$ for all $i \in I$, $\sum_{i \in I} p_i + \sum_{i \in I} q_i \leq 1$ and let $\{x_i\}_{i \in I}$ and $\{y_i\}_{i \in I}$ indexed collection such that $x_i, y_i \in X$ for all $i \in I$. If $X$ carries $\pca$ structure, then:
    $$
    \left(\bigboxplus_{i \in I} p_i \cdot x_i\right) \boxplus\left(\bigboxplus_{i \in I} q_i \cdot y_i\right) = \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i \right)
    $$
\end{lemma}
\begin{proof}
    Let $J = \{0,1\}$. Define indexed collections $\{r_{i,j}\}_{(i,j) \in I \times J}$ and $\{z_{i,j}\}_{(i,j) \in I \times J}$, such that $r_{i,0} = \frac{p_i}{p_i + q_i}$ and $z_{i,0} = x_i$ and $r_{i,1} = \frac{q_i}{p_i + q_i}$ and $z_{i,1} = x_i$.
    We have the following:
    \begin{align*}
        \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i\right) &= \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\bigboxplus_{j \in J} r_{i,j} \cdot z_{i,j} \right) \\
        &= \bigboxplus_{(i,j) \in I \times J} (p_i + q_i)r_{i_j} \cdot z_{i,j} \tag{\Cref{lem:flattening_convex_sums}} \\
        &= \left(\bigboxplus_{i \in I} p_i \cdot x_{i}\right) \boxplus \left(\bigboxplus_{i \in I} q_i \cdot y_i\right)
    \end{align*}
\end{proof}
Speaking more abstractly, positive convex algebras and their homomorphisms (in the sense of homomorphisms of algebras for the signature from universal algebra) form a category, that we will call $\pca$. This category can be seen as a concrete presentation of an Eilenberg-Moore algebra for the subdistribution monad.

\begin{theorem}\label{c4:thm:correspondence}
	There is an isomorphism of categories between $\pca$ and $\Set^{\distf}$. Given a set $X$ equipped with a positive convex algebra structure, we can define a map $h \colon \distf X \to X$, given by 
	$$
		h(\nu) = \bigboxplus_{x \in \supp(\nu)} \nu(x) \cdot x
	$$
	for all $\nu \in \distf X$, making $(X, h)$ into an algebra for the monad $\distf$. Equivalently, given a $\distf$-algebra $(X, h)$, one can define 
	$$
		\bigboxplus_{i \in I} p_i \cdot x_i = h \left(\sum_{i \in I} p_i \cdot \delta_{x_i}\right)
	$$
	for all finite $I$ and indexed collections $\{p_i\}_{i \in I}$, $\{x_i\}_{i \in I}$, such that $\sum_{i \in I} p_i \leq 1$ and for all $i \in I$, $x_i \in X$. This equips the set $X$ with a positive convex algebra structure.  
	\end{theorem}
	\begin{proof}
		See \cite[Theorem~4]{Jacobs:2010:Convexity} or \cite[Proposition~5.3]{Doberkat:2008:Erratum}.
	\end{proof}
	Moreover, $\pca$ as a category enjoys the following property: 
	\begin{theorem}[{\cite{Sokolova:2015:Congruences}}]
		In $\pca$ finitely presented and finitely generated objects coincide.
	\end{theorem}
	\subsection{Rational fixpoint}
	Let $B \colon \C \to \C$ be a finitary functor. We write $\coaf{B}$ for the subcategory of $\coa{B}$ consisting only of $B$-coalgebras with finitely presentable carrier. The \emph{rational fixpoint} is defined as $(\varrho B,r) = \colim(\coaf{B} \hookrightarrow \coa{B})$ -- a colimit of the inclusion functor from the subcategory of coalgebras with finitely presentable carriers. We call it a fixpoint, as the map $r \colon \varrho B \to B (\varrho B) $ is an isomorphism~\cite{Adamek:2006:Iterative}. 
% The rational fixpoint can be both viewed as a final locally finitely presentable $F$-coalgebra and an initial iterative $F$-algebra~\cite{Adamek:2006:Iterative}. 
Following~{\cite[Corollary~3.10, Theorem~3.12]{Milius:2020:New}}, if finitely presentable and finitely generated objects coincide in $\C$ and $B : \C \to \C$ is a finitary endofunctor preserving non-empty monomorphisms, then rational fixpoint is fully abstract - ie. $(\varrho B, r)$ is a subcoalgebra of the final coalgebra $(\nu B, t)$.

	\section{Operational semantics}
	We use {GPTS} to equip {PRE} with an operational semantics. More precisely, we define a function $\partial : \PExp \to \distf (1+A\times \PExp)$. We refer to $\partial$ as the \emph{Antimirov derivative}, as it is reminiscent of the analogous construction
for regular expressions and nondeterminisic automata due to Antimirov~\cite{Antimirov:1996:Partial}. Given $a \in A$, $e,f \in \PExp$ and $p \in [0,1]$ we define:
\begin{gather*}
    \partial(\zero) = 0 \quad\quad \partial(\one)=\delta_{\checkmark} \quad\quad \partial(a)=\delta_{(a,\one)}  \\  \partial(e \oplus_p f)=p\partial(e) + (1-p)\partial(f)
\end{gather*}
The expression $\zero$ is mapped to the empty support subdistribution, intuitively representing a deadlock. On the other hand, the expression $\one$ represents immediate acceptance, that is it transitions to $\checkmark$ with probability $1$. For any letter $a \in A$ in the alphabet, the expression $a$ performs $a$-labelled transition to $\one$ with probability $1$. The outgoing transitions of the probabilistic choice $e \oplus_p f$ consist of the outgoing transitions of $e$ with
probabilities scaled by $p$ and the outgoing transitions of $f$ scaled by $1-p$.

The definition of $\partial(e;f)$ is slightly more involved. We need to factor in the possibility that $e$ may accept with some probability $t$, in which case the outgoing transitions of $f$ contribute to the outgoing transitions of $e \seq f$. Formally, $\partial(e \seq f) = \partial(e) \lhd f$ where for any $f \in \PExp$ the operation $( - \lhd f) : \distf (1+A\times \PExp) \to \distf (1+A\times \PExp)$ is given by $( - \lhd f) = {c_f}^{\star}$, the convex extension of $c_f : 1+A\times \PExp \to \distf (1+A \times \PExp)$ given below on the left.
\begin{gather*}
c_f(x) = \begin{cases}
    \partial(f) & x = \checkmark \\
    \delta_{(a, e' \seq f)} & x = (a,e')
\end{cases}\qquad\qquad
\begin{tikzpicture}[baseline=-5ex]
        \node (0) {${e}\seq{f}$};
        \node (4) [below=.75cm of 0] {${e}'{ {} \seq {f}}$};
        \node (5) [left=0.5cm of 0] {{ \bcancel\checkmark}};
            \node (5a) [left=-.2cm of 5] {{ \(\partial({f})\)}};
        \draw (0) edge[-implies, double, double distance=0.5mm] node[above] {\({t}\)} (5);
        \draw (0) edge[-latex] node[left] {\footnotesize\( {a} \mid {s}\)} (4);
\end{tikzpicture}
\end{gather*}
Intuitively, $c_{f}$ reroutes the transitions coming out of ${e}$: acceptance (the first case) is replaced by the behaviour of ${f}$, and the probability mass of transitioning to ${e}'$ (the second case) is reassigned to $ e\seq f$.
A pictorial representation of the effect on the derivatives of ${e} \seq {f}$ is given above on the right.
Here, we assume that \(\partial({e})\) can perform a \({a}\)-transition to \({e'}\) with probability \({s}\); we make the same assumption in the informal descriptions of derivatives for the loops, below. 

For loops, we require $\partial\left(e^{[p]}\right)$ to be the least subdistribution satisfying $\partial\left(e^{[p]} \right) =  p \partial(e) \lhd e^{{[p]}} + (1-p) \partial(\checkmark)$. In the case when $\partial(e)(\checkmark)\neq 0$, the above becomes a fixpoint equation (as in such a case, the unrolling of the definition of $(- \lhd e^{[p]})$ involves $\partial \left( e{[p]}\right)$). We can define $\partial \left( e^{[p]} \right)$ as a closed form, but we need to consider two cases. If $\partial(e)(\checkmark)=1$ and $p = 1$, then the loop body is constantly executed, but the inner expression $e$ does not perform any labelled transitions. We identify such divergent loops with deadlock behaviour and hence $\partial(e^{[p]})(x)=0$. Otherwise, we look at $\partial(e)$ to build $\partial\left({e}^{[p]}\right)$. First, we make sure that the loop may be skipped with probability $1-p$.
Next, we modify the branches that perform labelled transitions by adding ${e}^{[p]}$ to be executed next.
The remaining mass is $p\partial(e)(\checkmark)$, the probability that we will enter the loop and immediately exit it without performing any labelled transitions. We discard this possibility and redistribute it among the remaining branches. 
As before, we provide an informal visual depiction of the probabilistic loop semantics below, using the same conventions as before. The crossed-out checkmark along with the dashed lines denotes the redistribution of probability mass described above.
\[
	\begin{tikzpicture}
        \node (0) {${e}^{[p]}$};
        \node (6) [below=.5cm of 0] {\checkmark};
        \node (4) [right=2cm of 0] {${e}'{ \seq {e}^{[{p}]}}$};
        \node (5) [left=.5cm of 0] {{ \(\bcancel{\checkmark}\)}};
        \draw (0) edge[-implies, double, double distance=0.5mm] node[left] {\footnotesize \(\frac{1-{p}}{1-{pt}}\)} (6);
        \draw (0) edge[-implies, double, double distance=0.5mm, pos=0.3] node[above, gray] {\({pt}\)} (5);
        \draw (0) edge[-latex] node[above] {\footnotesize\( {a} \mid {{{p}}}{s}/{ (1-{pt})}\)} (4);
        \draw (5) edge[->,dashed,bend left, out=90] ($(4.west) + (-0.5,0.35)$);
        \draw (5) edge[->,dashed,bend right, out=-90] ($(6.west) + (-0.5,0.35)$);
  %      \draw[dotted,bend right=20] (5) edge[-latex] (4);
   %     \draw[dotted,bend left=20] (5) edge[-latex] (6);
    \end{tikzpicture}
\]
Formally speaking, the definition of $\partial\left({e}^{[{p}]}\right)$ can be given by the following:
\begin{gather*}
\partial\left(e^{[p]}\right)(x)= \begin{cases}
    \frac{1-p}{1-p\partial(e)(\checkmark)} & x = \checkmark \\[1.2ex]
    \frac{p\partial(e)(a,e')}{1-p\partial(e)(\checkmark)} & x = (a, (e' \seq e^{[p]}))\\
    0 & \text{otherwise}
\end{cases}
\end{gather*}
% Having defined the semantics, we can easily observe that the termination operator $E : \PExp \to [0,1]$ captures the probability of immediate acceptance.
% \begin{restatable}{lemma}{exitoperatorlemma}\label{lem:exit_operator_lemma}
% For all $e \in \PExp$ it holds that $E(e)=\partial(e)(\checkmark)$
% \end{restatable}
% In our case, the canonical concept of coalgebraic bisimilarity (which in the case of {GPTS} directly corresponds to the notion from~\cite{Larsen:1991:Bisimulation}) is too discriminating notion of equivalence. For example, the states in the Brzozowski transition system for expressions $(a \oplus_{\frac{1}{2}} \zero) \seq (b \oplus_{\frac{1}{3}} \zero) $ and $(a \oplus_{\frac{1}{3}} \zero) \seq (b \oplus_{\frac{1}{2}} \zero) $ both generate a word $ab$ with probability $\frac{1}{6}$, while not being bisimilar. The state for the first expression can perform an $a$-transition with probability $\frac{1}{2}$, while the second one performs the same action with the probability of $\frac{1}{3}$, thus making them distinguished by the notion of bisimilarity.

% As mentioned before, if we omit the axioms marked with $\heartsuit$ from \cref{fig:axioms}, we obtain a coarser congruence relation ${\equiv_0} \subseteq \PExp \times \PExp$, sound wrt. coalgebraic bisimilarity. 
% % We will later use it in the proof of soundness of our axiomatisation wrt. language equivalence, as an intermediate step.
% \begin{restatable}{lemma}{soundnessbisim}\label{lem:soundness_bisim}
%     The relation ${\equiv_0} \subseteq {\PExp \times \PExp}$ is a bisimulation equivalence
% \end{restatable}
% As a consequence~\cite{Rutten:2000:Universal}, there exists a unique coalgebra structure $\ol{\partial} : {\PExp}/{\equiv_0} \to \distf F {\PExp}/{\equiv_0}$, which makes the quotient map $[-]_{\equiv_0} : \PExp \to {\PExp}/{\equiv_0}$ into $\distf F$-coalgebra homomorphism from $(\PExp, \equiv)$ to $({{\PExp}/{\equiv_0}}, \ol{\partial})$.

Having defined the Antimirov transition system, one can observe that the termination operator $E(-) \colon \PExp \to [0,1]$ precisely captures the probability of an expression transitioning to $\checkmark$ (successful termination) when viewed as a state in the Antimirov {GPTS}.

\begin{lemma}\label{lem:exit_operator_lemma}
    For all $e \in \PExp$ it holds that $E(e)=\partial(e)(\checkmark)$.
    \end{lemma}

Given an expression $e \in \PExp$, we write $\langle e\rangle \subseteq \PExp$ for the set of states reachable from $e$ by repeatedly applying $\partial$. In~\cite[Lemma~A.1]{preprint} in the appendix, we argue that this set is always finite - in other words, the operational semantics of every {PRE} can be always described by a finite-state {GPTS} given by $(\langle e \rangle, \partial)$.
\begin{example} Operational semantics of the expression $e = a \oplus_{\frac{3}{4}} a \seq a^{[\frac{1}{4}]}\seq a$ correspond to the following {GPTS}: 
\begin{center}
\begin{tikzpicture}%[baseline=0ex]
        \node (2) {$a \oplus_{\frac{3}{4}} a \seq a^{[\frac{1}{4}]}\seq a$};
        \node (2a) [ right= 2 cm of 2]{};
        \node (3) [below= .5 cm of 2a] {$a^{[\frac{1}{4}]}\seq a$};
        \draw (2) edge[-latex] node[below] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \draw (3) edge[-latex,out=-20,in=0,looseness=6] node[right] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \node (4) [right= 4.5 cm of 2] {$\one$};
        \draw (3) edge[-latex] node[below] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \draw (2) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \node (o2) [right= 0.75 cm of 4]{$\checkmark$};
        \draw (4) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{center}
One can observe that the transition system above for $e$ is isomorphic to the one starting in $q_2$ in \cref{ex:systems}.
\end{example}

Given the finite-state {GPTS} $(\langle e \rangle, \partial)$ associated with an expression $e\in \PExp$ we can define the language semantics of $e$ as the probabilistic language $\llbracket e\rrbracket \in [0,1]^{A^*}$ generated by the state $e$ in the {GPTS} $(\langle e \rangle, \partial)$. 
	\section{Language semantics}

 