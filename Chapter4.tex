\chapter{Completeness Theorem for Probabilistic Regular Expressions}
\label{chapter4}
\section{Overview}
In this section, we will introduce the syntax and the language semantics of probabilistic regular expressions ({PRE}), as well as a candidate inference system to reason about the equivalence of {PRE}. 

\subsection{Syntax}
Given a finite alphabet $\alphabet$, the syntax of {PRE} is given by:
$$e,f \in \PExp ::= \zero \mid \one \mid a \in A \mid e \oplus_p f \mid e\seq f \mid e^{[p]} \quad\quad\quad p \in\interval{0}{1}$$
We denote the expressions that immediately abort and successfully terminate by $\zero$ and $\one$ respectively. For every letter $a \in \alphabet$ in the alphabet, there is a corresponding expression representing an atomic action. Given two expressions $e, f \in {\PExp}$ and $p \in \interval{0}{1}$, probabilistic choice $e \oplus_p f$ denotes an expression that performs $e$ with probability $p$ and performs $f$ with probability $1-p$. One can think of $ \oplus_p$ as the probabilistic analogue of the
 plus operator ($e + f$) in Kleene's regular expressions. $e \seq f$ represents sequential composition, while $e^{[p]}$ is a probabilistic analogue of Kleene star: it successfully terminates with probability $1-p$ or with probability $p$ performs $e$ and then iterates $e^{[p]}$ again.  In terms of the notational convention, the sequential composition $(\seq)$ has higher precedence than the probabilistic choice $(\oplus_p)$.

\begin{example}
    The expression $a \seq a^{[\frac{1}{4}]}$ first performs action $a$ with probability $1$ and then enters a loop which successfully terminates with probability $\frac{3}{4}$ or performs action $a$ with probability $\frac{1}{4}$ and then repeats the loop again. Intuitively, if we think of the action $a$ as observable, the expression above denotes a probability associated with a non-empty sequence of $a$'s. For example, the sequence $aaa$ would be observed with probability $1 \cdot (1/4)^2 \cdot 3/4=3/64$.
\end{example}
\subsection{Language semantics}
{PRE} denote probabilistic languages $A^* \to [0,1]$. For instance, the expression $\zero$ denotes a function that assigns $0$ to every word, whereas $\one$ and $a$ respectively assign probability $1$ to the empty word and the word containing a single letter $a$ from the alphabet. The probabilistic choice $e \oplus_p f$ denotes a language in which the probability of each word is the total sum of its probability in $e$ scaled by $p$ and its probability in $f$ scaled by $1-p$. Describing the semantics of sequential composition and loops inductively is more involved. In particular, the semantics of loops would require a fixpoint calculation, which does not have as clear and straightforward (closed-form) formula, as the asterate of regular languages. Instead, we take an \emph{operational approach}, and we formally define the language semantics of {PRE} in \Cref{sec:language_semantics} through a small-step operational semantics, using a specific type of probabilistic transition system, which we introduce next.

\subsection{Generative probabilistic transition systems}
A {GPTS} consists of a set of states $Q$ and a transition function that maps each state $q\in Q$ to finitely many distinct outgoing arrows of the form:
\begin{itemize}
    \item \emph{successful termination} with probability $t$ (denoted $q \xRightarrow[]{t} \checkmark$), or 
    \item to another state $r$, via an \emph{$a$-labelled transition}, with probability $s \in [0,1] $ (denoted $q \xrightarrow[]{a \mid s} r$).
\end{itemize} 
We require that, for each state, the total sum of probabilities appearing on outgoing arrows sums up to less or equal to one. The remaining probability mass is used to model unsuccessful termination, hence the state with no outgoing arrows can be thought of as exposing deadlock behaviour. 

Given a word $w \in A^*$ the probability of it being generated by a state $q\in Q$ (denoted $\llbracket q\rrbracket (w) \in [0,1]$) is defined inductively:
\begin{equation}\label{language}
    \llbracket q\rrbracket (\epsilon)=t \quad\text{if } q \xRightarrow[]{t} \checkmark \qquad\qquad
    \llbracket q \rrbracket(av) = \sum_{q \xrightarrow[]{a \mid s} r} s \cdot \llbracket r \rrbracket(v)
\end{equation}
We say that two states $q$ and $q'$ are \emph{language equivalent} if for all words $w \in A^*$, we have that $\llbracket q \rrbracket(w)=\llbracket q' \rrbracket(w)$.
\begin{example}\label{ex:systems}
 Consider the following {GPTS}:
 \begin{gather*}
\begin{tikzpicture}[baseline=-3ex]
        \node (0) {$q_0$};
        \node (1) [right= 1.15 cm of 0]{$q_1$};
        \draw (0) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {1}\)} (1);
        \draw (1) edge[loop above] node[left] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (1);
        \node (o1) [right= 0.5 cm of 1]{$\checkmark$};
        \draw (1) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( \frac{3}{4}\)} (o1);
        \node (2) [right= 0.5 cm of o1] {$q_2$};
         \node (2a) [right= 1.25 cm of 2] {};
        \node (3) [below= -.01 cm of 2a] {$q_3$};
        \draw (2) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \draw (3) edge[loop below] node[right] {\scriptsize\( {a} \mid {\frac{1}{4}}\)} (3);
        \node (4) [right= 3 cm of 2] {$q_4$};
        \draw (3) edge[-latex] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \draw (2) edge[-latex, bend left] node[ fill=white] {\scriptsize\( {a} \mid {\frac{3}{4}}\)} (4);
        \node (o2) [right= 0.5 cm of 4]{$\checkmark$};
        \draw (4) edge[-implies, double, double distance=0.5mm] node[above] {\scriptsize\( 1\)} (o2);
\end{tikzpicture}
\end{gather*}
%States $q_0$ and $q_2$ despite not being bisimilar (in the sense from~\cite{Larsen:1991:Bisimulation}), 
States $q_0$ and $q_2$ both assign probability $0$ to the empty word $\epsilon$ and each word $a^{n+1}$ is mapped to the probability $\left(\frac{1}{4}\right)^n \times \frac{3}{4}$. Later in the paper, we show that the languages generated by states $q_0$ and $q_2$ can be specified using expressions $a\seq a^{\left[ \frac{1}{4} \right]}$ and $a \oplus_{\frac{3}{4}} ( a\seq a^{\left[ \frac{1}{4} \right]} \seq a)$ respectively.
\end{example}
In \Cref{sec:operational_semantics}, we will associate to each {PRE} $e$ an operational semantics or, more precisely, a state $q_e$ in a {GPTS}. The language semantics of $e$ will then be the language $\llbracket q_e \rrbracket \colon A^* \to [0,1]$ generated by $q_e$. Two {PRE} $e$ and $f$ are language equivalent if $\llbracket q_e \rrbracket=\llbracket q_f \rrbracket$. One of our main goals is to present a complete inference system to reason about language equivalence. In a nutshell, we want to present a system of (quasi-)equations of the form $e\equiv f$ such that:
\[
e\equiv f \Leftrightarrow \llbracket q_e \rrbracket=\llbracket q_f \rrbracket
\]
Such an inference system will have to contain rules to reason about all constructs of {PRE}, including probabilistic choice and loops. We describe next the system, with some intuition for the inclusion of each group of rules. 

\vspace{2px}
\noindent
\textbf{Axiomatisation of language equivalence of {PRE}.} We define ${\equiv} \subseteq \Exp \times \Exp$ to be the least congruence relation closed under the axioms shown on \cref{fig:axioms}. We will show in \Cref{sec:completeness} that these axioms are complete wrt. language semantics. 

\begin{figure*}
        \centering
        \begin{multicols}{2}
        	 \begin{flushleft}\underline{\bf Probabilistic Choice}  	 \end{flushleft}
        \begin{alignat*}{4}
		\textbf{(C1)} & \;\, 
		& e &{} \equiv e \oplus_p e\\
				\textbf{(C2)} & \;\, 
		& e &{} \equiv e \oplus_1 f\\
		\textbf{(C3)} & \;\, 
		& e \oplus_p  f &{} \equiv f \oplus_{\overline{p}} e\\
		\textbf{(C4)} & \;\, 
		& (e \oplus_{p} f) \oplus_{q} g & {} \equiv e \oplus_{pq} (f \oplus_{\frac{\overline{p}q}{1-pq}} g)\\
		\textbf{(D1)} & \;\, 
		& (e \oplus_{p} f) \seq g & {} \equiv e \seq g \oplus_{p} f \seq g\\[-0.25ex]
		\textbf{(D2)} & \;\, 
		& e \seq (f \oplus_{p} g) & {} \equiv e \seq g \oplus_{p} e \seq f\\[-0.25ex]\\\\
        \end{alignat*}
          \begin{flushleft}   \underline{\bf Sequencing} \end{flushleft}
        \begin{alignat*}{4}
        \textbf{(0S)} & \;\, 
		& 0 \seq e & {} \equiv 0 & \\[-0.25ex]
		\textbf{(S0)} & \;\, 
		& e \seq 0 & {} \equiv 0 & \\[-0.25ex]
		\textbf{(1S)} & \;\, 
		& 1 \seq e & {} \equiv e & \\[-0.25ex]
		\textbf{(S1)} & \;\, 
		& e \seq 1 & {} \equiv e \\[-0.25ex]
		\textbf{(S)} & \;\, 
		& e \seq (f \seq g) & {} \equiv (e \seq f ) \seq g \\[-0.25ex]
        \end{alignat*}
        \end{multicols}
        \vspace{-2cm}
         \begin{multicols}{2}
     \begin{flushleft}    \underline{\bf Loops} \end{flushleft}
        \begin{alignat*}{4}
       	\textbf{(Unroll)} & \;\, 
		& e \seq e^{[p]} \oplus_p 1& {} \equiv e^{[p]} \\
		\textbf{(Tight)} & \;\, 
		& (e \oplus_p 1)^{[q]} \seq 1 & {} \equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]} \\[-0.25ex]
		\textbf{(Div)} & \;\, 
		& 1^{[1]} & {} \equiv 0 \\
%		 \\ \quad\;\;\textbf{(Unique)} \;\, 
%		\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}
        \end{alignat*}
          \begin{flushleft}   \underline{\bf (Unique) fixpoint rule} \end{flushleft}
		$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$
\end{multicols}
        \vspace{-.5cm}
     \begin{flushleft}    \underline{\bf Termination cond. $E : \Exp \to [0,1]$} \end{flushleft}
        \fcolorbox{red}{white}{$
        \begin{array}{l}
            \quad E(\one)=1 \qquad E(\zero)=E(a) = 0 \qquad
            E \left( e \oplus_p f \right)=pE(e) + \ol{p}E(f)\qquad 
            E(e\seq f)=E(e)E(f)\quad \\[1.4ex]
            \quad E\left(e^{[p]}\right)=
                \begin{cases}
                0 &  \text{\scriptsize $E(e)=1 \wedge p=1$} \\ 
                \frac{1-p}{1-pE(e)} & \text{\scriptsize otherwise}
                \end{cases}   
                \end{array}$
     }
     %        \end{tabular}
      %		\begin{align*}
%            \textbf{(Unroll)} \quad e^{[p]} &\equiv e \seq e^{[p]}\\
%            \textbf{(Tight)} \quad (e \oplus_p \one)^{[q]} &\equiv e^{\left[\frac{pq}{1-\ol{p}q}\right]}\\
%            \one^{[1]} &\equiv \zero &\textbf{(Div)}\\
%           \omit\rlap{$\inferrule{g \equiv e\seq g \oplus_p f \quad \fcolorbox{red}{white}{$E(e)=0$}}{g \equiv e^{[p]}\seq f}$}&&\textbf{(Unique)}\\
%        \end{align*}
        \caption{\label{fig:axioms} For $p \in [0,1]$, we write $\ol{p}=1-p$. The rules involving the division of probabilities are defined only when the denominator is non-zero. The function $E(-)$ provides a termination side condition to the \textbf{Unique} fixpoint axiom.}
            \vspace{-.4cm}
    \end{figure*}
%We will also write ${\equiv_0} \subseteq {\Exp \times \Exp}$ for the least congruence relation containing all the above rules except the two marked with $\heartsuit$. Intuitively, removing these rules axiomatises bisimilarity, which is the finer notion of equivalence. This relation will be used in the intermediate step of the proof of soundness. We will use the following notation for the quotient maps:
%% https://q.uiver.app/?q=WzAsMyxbMSwwLCJ7XFxFeHB9L3tcXGVxdWl2XzB9Il0sWzIsMCwie1xcRXhwfS97XFxlcXVpdn0iXSxbMCwwLCJcXEV4cCJdLFswLDEsIlstXV97XFxlcXVpdn0iLDJdLFsyLDAsIlstXV97XFxlcXVpdl8wfSIsMl0sWzIsMSwiWy1dIiwwLHsiY3VydmUiOi0zfV1d
%\[\begin{tikzcd}
%	\Exp & {{\Exp}/{\equiv_0}} & {{\Exp}/{\equiv}}
%	\arrow["{[-]_{\equiv}}"', from=1-2, to=1-3]
%	\arrow["{[-]_{\equiv_0}}"', from=1-1, to=1-2]
%	\arrow["{[-]}", curve={height=-18pt}, from=1-1, to=1-3]
%\end{tikzcd}\]

The first group of axioms capture properties of the probabilistic choice operator $\oplus_p$ (\textbf{C1-C4}) and its interaction with sequential composition (\textbf{D1-D2}). Intuitively, \textbf{C1-C4} are the analogue of the semilattice axioms governing the behaviour of $+$ in regular expressions. These four axioms are reminiscent of the axioms of barycentric algebras~\cite{Stone:1949:Postulates}. \textbf{D1} and \textbf{D2} are \emph{right and left distributivity} rules of $\oplus$ over $\seq$. The sequencing axioms \textbf{1S, S1, S}  state {PRE} have the structure of a monoid (with neutral element $\one$) with absorbent element $\zero$ (\textbf{0S, S0}). The loop axioms contain respectively \emph{unrolling}, \emph{tightening}, and \emph{divergency} axioms plus a \emph{unique fixpoint} rule. The \textbf{Unroll} axiom associates loops with their intuitive behaviour of choosing, at each step, probabilistically between successful termination and executing the loop body once. \textbf{Tight} and \textbf{Div} are the probabilistic analogues of the identity $(e + \one)^{*} \equiv e^{*}$ from regular expressions. In the case of {PRE}, we need two axioms: \textbf{Tight} states that the probabilistic loop whose body might instantly terminate, causing the next loop iteration to be executed immediately is provably equivalent to a different loop, whose body does not contain immediate termination; \textbf{Div} takes care of the edge case of a no-exit loop and identifies it with failure. Finally, the unique fixpoint rule is a re-adaptation of the analogous axiom from Salomaa's axiomatisation and provides a partial converse to the loop unrolling axiom, given the loop body is productive -- i.e. cannot immediately terminate. This productivity property is formally written using the side condition $E(e) = 0$, which can be thought of as the probabilistic analogue of empty word property from Salomaa’s axiomatisation. Consider an expression $a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} 1)$. The only way it can accept the empty word is to leave the loop with the probability of $\frac{1}{2}$ and then perform $1$, which also can happen with probability $\frac{1}{2}$. In other words, $\llbracket a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} 1)\rrbracket(\epsilon) = \frac{1}{4}$. A simple calculation allows to verify that $E( a^{\left[\frac{1}{2}\right]} \seq (b \oplus_{\frac{1}{2}} 1)) = \frac{1}{4}$.




\begin{example} We revisit the expressions from \cref{ex:systems} and show their equivalence via axiomatic reasoning.
\begin{align*}
     a \seq a^{[\frac{1}{4}]} &\stackrel\dagger\equiv   a \seq (a^{[\frac{1}{4}]} \seq a \oplus_{\frac{1}{4}} \one) \stackrel{\textbf{D2}}\equiv  (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \seq \one\\  &\stackrel{\textbf{S1}}\equiv (a \seq a^{[\frac{1}{4}]}\seq a) \oplus_{\frac{1}{4}} a \stackrel{\textbf{C3}}\equiv a \oplus_{\frac{3}{4}} (a \seq a^{[\frac{1}{4}]}\seq a)
\end{align*}

The $\dagger$ step of the proof above relies on the equivalence $e^{[p]}\seq e \oplus_p \one \equiv e$ derivable from other axioms under the assumption $E(e)=0$ through a following line of reasoning:
\begin{align*}
    e^{[p]} \seq e \oplus_p \one  &\equiv (e \seq e^{[p]} \oplus_p \one)\seq e\oplus_p \one \tag{\textbf{Unroll}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p \one\seq e) \oplus_p \one \tag{\textbf{D1}}\\
    &\equiv (e \seq e^{[p]}\seq e \oplus_p e) \oplus_p \one \tag{\textbf{1S}}\\
        &\equiv (e \seq e^{[p]}\seq e \oplus_p e\seq \one) \oplus_p \one \tag{\textbf{S1}}\\
    &\equiv e \seq (e^{[p]}\seq e \oplus_p \one) \oplus_p \one \tag{\textbf{D2}}
\end{align*}
Since $E(e)=0$,  we then have: 
\(
     e^{[p]} \seq e \oplus_p \one \stackrel{(\textbf{Unique})}\equiv e^{[p]}\seq \one 
     \stackrel{(\textbf{S1})}\equiv e^{[p]} .
\)
\end{example}
\section{Preliminaries}
\label{c4:sec:preliminaries}
\subsection{Monads and their algebras}\label{c4:subsec:monads}
A monad (over the category $\Set$) is a triple $\monadT = (T, \mu, \eta)$ consisting of a functor $T \colon \Set \to \Set$ and two natural transformations: a unit $\eta \colon \Id \Rightarrow T$ and multiplication $\mu \colon T^2 \Rightarrow T$ satisfying $\mu \circ \eta_T = \id_T=  \mu \circ T\eta$ and $\mu \circ \mu_T = \mu \circ T \mu$
A $\monadT$-algebra (also called an Eilenberg-Moore algebra) for a monad $T$ is a pair $(X, h)$ consisting of a set $X \in \Obj(\C)$, called carrier, and a function $h \colon T X \to X$ such that $h \circ \mu_X = h \circ Th$ and $h \circ \eta_X = \id_X$. A $\monadT$-algebra homomorphism between two $T$-algebras $(X,h)$ and $(Y,k)$ is a function 	$f \colon X \to Y$ satisfying $k \circ Tf = f \circ h$.

$\monadT$-algebras and $\monadT$-homomorphisms form a category $\Set^\monadT$. There is a canonical forgetful functor $\forget \colon \Set^\monadT \to \Set$ that takes each $\monadT$-algebra to its carrier. This functor has a left adjoint $X \mapsto (TX, \mu_X \colon T^2 X \to T)$, mapping each set to its free $\monadT$-algebra. If $X$ is finite, then we call $(TX, \mu_X)$ free finitely generated.

Given a function $f \colon X \to Y$, where $Y$ is a carrier of a $\monadT$-algebra $(Y, h)$, there is a unique homomorphism $f^\sharp \colon (TX, \mu_X) \to (Y,h)$ satisfying $f^\sharp \circ \eta_X = f$ that is explicitly given by $f^\sharp = h \circ Tf$.

\subsection{Generalised determinisation}\label{sec:c4:generalised_determinisation}
Language acceptance of nondeterministic automata (NDA) can be captured via determinisation. {NDA} can be viewed as coalgebras for the functor $N = 2 \times {\powf}^\alphabet$, where $\powf$ is the finite powerset monad. Determinisation converts a {NDA} $(X, \beta \colon X \to 2 \times {\powf X}^\alphabet)$ into a deterministic automaton $\left({\powf X}, \beta^{\sharp} \colon {\powf X} \to 2 \times {(\powf X)}^\alphabet \right)$, where for $A \subseteq X$, we define $\beta^{\sharp}(A) = \bigcup_{x \in A} \beta(a)$. Additionally, $\beta^\sharp$ satisfies $\beta^\sharp(\{x\}) = \beta(x)$ for all $x \in X$. A language of the state $x \in X$ of {NDA}, is given by the language accepted by the state $\{x\}$ in the determinised automaton $({\powf X}, \beta^\sharp)$. 


This construction can be generalised~\cite{Silva:2010:Generalizing} to $HT$-coalgebras, where $T \colon \Set \to \Set$ is an underlying functor of finitary monad $\monadT$ and $H \colon \Set \to \Set$ an endofunctor that admits a final coalgebra that can be lifted to the functor $\overline{H} \colon \Set^\monadT \to \Set^\monadT$. Liftings of functors $H \colon \Set \to \Set$ to $\overline{H} \colon \Set^\monadT \to \Set^\monadT$, are in one-to-one correspondence with distributive laws of the monad $\mathsf{T}$ over the functor $H$~\cite{Jacobs:2015:Trace}, which are natural transformations $\lambda \colon TH \Rightarrow HT$ satisfying
$H\eta_X = \lambda_X \circ \eta_{HX}$ and $H \mu_X \circ \lambda_{TX} \circ T\lambda_X = \lambda_X \circ \mu_{HX}$. 

Generalised determinisation turns $HT$-coalgebras $(X, \beta \colon X \to HT X)$ into $H$-coalgebras $(T X , \beta^\sharp \colon T X \to H T X)$, where $\beta^\sharp$ is the unique extension arising from the free-forgetful adjunction between $\Set$ and $\Set^\monadT$. The language of a state $x \in X$ is given by $\beh{\beta^\sharp} \circ \eta_X \colon X \to \nu H$, where $\eta$ is the unit of the monad $T$. Since $\beta^\sharp \colon TX \to HTX$ can be seen as a $\monadT$-algebra homomorphism $(TX, \mu_X) \to \overline{H}(TX, \mu_X)$, each determinisation $(TX, \beta^{\sharp})$ can be viewed as an $\overline{H}$-coalgebra $((TX, \mu_X), \beta^{\sharp})$. The carrier of the final $H$-coalgebra can be canonically equipped with $\monadT$-algebra structure, yielding the final $\overline{H}$-coalgebra. In such a case, the unique final homomorphism from any determinisation (viewed as an $H$-coalgebra) is precisely an underlying function of the final $\overline{H}$-coalgebra homomorphism. 

\subsection{Subdistribution monad}\label{c4:subsec:subdistribution}
 A function $\nu \colon X \to [0,1]$ is called a subprobability distribution or subdistribution, if it satisfies $\sum_{x \in X} \nu(x) \leq 1$. A subdistribution $\nu$ is \emph{finitely supported}  if the set $\supp(\nu) = \{x \in X \mid \nu(x) > 0\}$ is finite. We use $\distf {X}$ to denote the set of finitely supported subprobability distributions on $X$. Given a function $f \colon X \to Y$, we can define a map $\distf f \colon \distf X \to \distf Y$ given by 
 
 Given $x \in X$, its \emph{Dirac} is a subdistribution $\delta_x$ which is given by $\delta_x(y)=1$ only if $x=y$, and $0$ otherwise. We will moreover write $\emptydist \in \distf X$ for a subdistribution with an empty support. It is defined as $\emptydist(x)=0$ for all $x \in X$. When $\nu_1, \nu_2 \colon X \to \interval{0}{1}$ are subprobability distributions and $p \in \interval{0}{1}$, we write $p\nu_1 + (1-p)\nu_2$ for the convex combination of $\nu_1$ and $\nu_2$, which is the probability distribution given by $$(p \nu_1 + (1-p) \nu_2)(x) = p\nu_1(x) + (1-p)\nu_2(x)$$
 for all $x \in X$. Note that this operation preserves finite support. 
 
  $\distf$ is in fact a functor on the category $\Set$, which maps each set $X$ to $\distf X$ and maps each arrow $f \colon X \to Y$ to the function $\distf f \colon \distf X \to \distf Y$ given by $$\distf f (\nu)(x) = \sum_{y \in f^{-1}(x)} \nu(y)$$
   Moreover, $\distf$ also  carries a monad structure with unit  $\eta_X(x) = \delta_x$ and multiplication $\mu_X(\Phi)(x) = \sum_{\varphi \in \distf X }\Phi(\varphi)\varphi(x)$ for $\Phi \in \distf^2 X$. Using the free-forgetful adjunction between $\Set$ and category of $\distf$-algebras, given $f \colon X \to \distf Y$, there exists a unique map $f^\sharp \colon \distf X \to \distf Y$ satisfying $f = f^\sharp \circ \delta$ called the \emph{convex extension of $f$}, and explicitly given by $f^\sharp(\nu)(y) = \sum_{x \in X} \nu(x) f(x)(y)$.
\subsection{Positive convex algebras}\label{c4:subsec:positive}
By $\sigpca$ we denote a signature given by
$$\sigpca = \left\{\bigboxplus_{i \in I} p_i \cdot (-)_i \mid I \text{ finite}, \forall i \in I \ldotp p_i \in [0,1], \sum_{i \in I} p_i \leq 1\right\} $$
A positive convex algebra is a an algebra for the signature $\sigpca$, that is a pair $\A = \left(X, \sigpca^\A\right)$, where $X$ is the carrier set and $\sigpca^\A$ is a set of interpretation functions $\bigboxplus_{i \in I} p_i \cdot (-)_i \colon X^{|I|} \to X$ satisfying the axioms:
\begin{enumerate}
    \item (Projection) \(\bigboxplus_{i \in I} p_i \cdot x_i = x_j\) if \(p_j=1\)
    \item (Barycenter) \(\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J}{q_{i,j}} \cdot {x_j}\right) = \bigboxplus_{j \in J} \left(\sum_{i \in I} p_i q_{i,j} \right) \cdot x_j\)
\end{enumerate}
In terms of notation, we denote the unary sum by $p_0 \cdot x_0$. Throughout this chapter we will we abuse the notation by writing $$\left(\bigboxplus_{i \in I} p_i \cdot e_i\right) \boxplus \left(\bigboxplus_{i \in J} q_j \cdot f_j\right)$$ for a single sum $\bigboxplus_{k \in I + J} r_k \cdot g_k$, where $r_k = p_k$ and $g_k = e_k$ for $k \in I$ and similarly $r_k = q_k$ and $g_k = f_k$ for $k \in J$. Note that this is well-defined only if $\sum_{i \in I} p_i + \sum_{j \in J} r_j \leq 1$.

The signature of positive convex algebras can be alternatively presented as a family of binary operations, in the following way:

\begin{proposition}\label{c4:prop:binary}
    If  $X$ is a set equipped with a binary operation $\boxplus_{p} : X \times X \to X$ for each $p \in [0,1]$ and a constant $0_\boxplus \in X$ satisfying for all $x,y,z \in X$ (when defined) the following:
    \begin{gather*}
        x \boxplus_{p} x = x \qquad x \boxplus_{1} y = x \qquad x \boxplus_{p} y = y \boxplus_{1-{p}} x \\ (x \boxplus_{p} y) \boxplus_{q} z = x \boxplus_{pq} \left(y \boxplus_{\frac{(1-p)q}{1-pq}} z\right)
    \end{gather*}
    then $X$ carries the structure of a positive convex algebra. The interpretation of $\boxplus_{i \in I} p_i \cdot (-)_{i}$ is defined inductively by the following
    $$\bigboxplus_{i \in I} p_i \cdot x_i = \begin{cases}
        0_\boxplus & \text{if } I = \emptyset \\
        x_0 & \text{if } p_0 = 1\\
        x_n \boxplus_{p_k} \left(\bigboxplus_{i \in I \setminus \{k\}} \frac{p_i}{1-{p_k}}\cdot x_i  \right) &\text{otherwise, for some } k \in I
    \end{cases} $$
\end{proposition}
%\begin{proof}
%    A straightforward re-adaptation of \cite[Proposition~7]{Bonchi:2017:Power}.
%\end{proof}
Below we state several properties of positive convex algebras, that we will use throughout this chapter.
\begin{proposition}\label{c4:prop:properties_of_positive_convex_algebras}
    Let $I$ be a finite indexed set, and let $\{p_i\}_{i \in I}$ and $\{x_i\}_{i \in I}$ be indexed collections of elements of $\interval{0}{1}$ and $X$ respectively. Then, in any positive convex algebra, the following statements hold:
    \begin{enumerate}
        \item
        $${\bigboxplus_{i \in I} p_i \cdot x_i} = {\bigboxplus_{x \in \bigcup_{i \in I} \{x_i\}} \left(\sum_{x_i = x} p_i\right)\cdot x}$$
        \item Let ${=_R} \subseteq {X \times X}$ be a congruence relation, with $[-]_R \colon X \to X/{=_R}$ being its canonical quotient map. Then, 
        $${\bigboxplus_{i \in I} p_i \cdot x_i} =_R {\bigboxplus_{[x]_R \in \bigcup_{i \in I} \left\{[x_i]_R\right\}} \left(\sum_{x_i =_R x} p_i\right)\cdot x}$$
        \item All terms $\bigboxplus_{i \in I } 0 \cdot x_i $ coincide and are all provably equivalent to the empty convex sum.
        \item If $J \subseteq I$ and $J \supseteq \{i \in I \mid p_i \neq 0\}$, then 
        $$
        \bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{j \in J} p_j \cdot x_j
        $$
        \item Let $\sigma \colon I \to I$ be a permutation of the index set $I$. Then, we have that
        $$
        	\bigboxplus_{i \in I} p_i \cdot x_i = \bigboxplus_{i \in I} p_{\sigma(i)} \cdot x_{\sigma(i)}
        $$
    \end{enumerate}
\end{proposition}
\begin{proof}
    We write $[\Phi]$ to denote Iverson bracket, which is defined to be $1$ if $\Phi$ is true and $0$ otherwise.
    
    For \circlednum{1} we have that
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot x_i &= \bigboxplus_{i \in I} p_i \cdot \left( \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} [x_i = x] \cdot  x \right) &\tag{Projection axiom}\\
        &= \bigboxplus_{x \in \cup_{i \in I} \{x_i\}} \left( \sum_{i \in I} p_i[x_i = x] \right) \cdot x \tag{Barycenter axiom} \\
        &=  \bigboxplus_{x \in \cup_{i \in I} \{x_i\} } \left(\sum_{x_i = x} p_i\right) \cdot x
    \end{align*}
    \circlednum{2} can be shown by picking a representative for each equivalence class and then using \circlednum{1}. For \circlednum{3}, by \cite[Lemma~3.4]{Sokolova:2015:Congruences} we know that all terms $\bigboxplus_{i \in I} 0 \cdot x_i$ coincide. To see that they are provably equivalent to the empty convex sum, observe that
    \begin{align*}
        \bigboxplus_{i \in I} 0 \cdot x_i &= \bigboxplus_{i \in I} 0 \cdot \left(\bigboxplus_{j \in \emptyset} p_j \cdot y_j\right)\tag{\cite[Lemma~3.4]{Sokolova:2015:Congruences}}\\
        &= \bigboxplus_{j \in \emptyset} 0 \cdot y_j \tag{Barycenter axiom}\\
    \end{align*}
    Finally, \circlednum{4} follows from \cite[Lemma~3.4]{Sokolova:2015:Congruences}, while \circlednum{5} was proved in \cite[Proposition~3.1]{Doberkat:2008:Erratum}.
\end{proof}
\begin{lemma}\label{lem:flattening_convex_sums}
    Let $I, J$ be finite index sets, $\left\{p_i\right\}_{i \in I}$, $\{q_{i,j}\}_{(i,j) \in I\times J}$ and $\{x_{i,j}\}_{(i,j) \in I \times J}$ indexed collections such that for all $i \in I$ and $j \in J$, $p_i, q_{i,j} \in [0,1]$ and $x_{i,j} \in X$. If $X$ carries $\pca$ structure, then:
    $$\bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) = \bigboxplus_{(i,j) \in I \times J} p_iq_{i,j} \cdot x_{i,j}$$
\end{lemma}
\begin{proof}
    \begin{align*}
        \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{j \in J} q_{i,j} \cdot x_{i,j}\right) &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in \{i\} \times J} q_{k,j} \cdot x_{k,j}\right) \\
        &= \bigboxplus_{i \in I} p_i \cdot \left(\bigboxplus_{(k,j) \in I \times J} [k = i]q_{k,j} \cdot x_{k,j}\right) \tag{\Cref{c4:prop:properties_of_positive_convex_algebras}} \\
        &= \bigboxplus_{(k,j) \in I \times J} \left(\sum_{i \in I} p_i [k=i]q_{k,j} \right) \cdot x_{k,j} \tag{Barycenter axiom} \\
        &= \bigboxplus_{(k,j) \in I \times J} p_k q_{k,j}  \cdot x_{k,j} \\
        &= \bigboxplus_{(i,j) \in I \times J} p_i q_{i,j}  \cdot x_{i,j} \tag{Renaming indices}\\
    \end{align*}
\end{proof}
\begin{lemma}\label{c4:lem:grouping_probabilities}
    Let $I$ be a finite index set, $\{p_i\}_{i \in I}$ and $\{q_i\}_{i \in I}$ indexed collections such that $p_i,q_i \in [0,1]$ for all $i \in I$, $\sum_{i \in I} p_i + \sum_{i \in I} q_i \leq 1$ and let $\{x_i\}_{i \in I}$ and $\{y_i\}_{i \in I}$ indexed collection such that $x_i, y_i \in X$ for all $i \in I$. If $X$ carries $\pca$ structure, then:
    $$
    \left(\bigboxplus_{i \in I} p_i \cdot x_i\right) \boxplus\left(\bigboxplus_{i \in I} q_i \cdot y_i\right) = \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i \right)
    $$
\end{lemma}
\begin{proof}
    Let $J = \{0,1\}$. Define indexed collections $\{r_{i,j}\}_{(i,j) \in I \times J}$ and $\{z_{i,j}\}_{(i,j) \in I \times J}$, such that $r_{i,0} = \frac{p_i}{p_i + q_i}$ and $z_{i,0} = x_i$ and $r_{i,1} = \frac{q_i}{p_i + q_i}$ and $z_{i,1} = x_i$.
    We have the following:
    \begin{align*}
        \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\frac{p_i}{p_i + q_i} \cdot x_i \boxplus \frac{q_i}{p_i + q_i} \cdot y_i\right) &= \bigboxplus_{i \in I} (p_i + q_i) \cdot \left(\bigboxplus_{j \in J} r_{i,j} \cdot z_{i,j} \right) \\
        &= \bigboxplus_{(i,j) \in I \times J} (p_i + q_i)r_{i_j} \cdot z_{i,j} \tag{\Cref{lem:flattening_convex_sums}} \\
        &= \left(\bigboxplus_{i \in I} p_i \cdot x_{i}\right) \boxplus \left(\bigboxplus_{i \in I} q_i \cdot y_i\right)
    \end{align*}
\end{proof}
Speaking more abstractly, positive convex algebras and their homomorphisms (in the sense of homomorphisms of algebras for the signature from universal algebra) form a category, that we will call $\pca$. This category can be seen as a concrete presentation of an Eilenberg-Moore algebra for the subdistribution monad.

\begin{theorem}\label{c4:thm:correspondence}
	There is an isomorphism of categories between $\pca$ and $\Set^{\distf}$. Given a set $X$ equipped with a positive convex algebra structure, we can define a map $h \colon \distf X \to X$, given by 
	$$
		h(\nu) = \bigboxplus_{x \in \supp(\nu)} \nu(x) \cdot x
	$$
	for all $\nu \in \distf X$, making $(X, h)$ into an algebra for the monad $\distf$. Equivalently, given a $\distf$-algebra $(X, h)$, one can define 
	$$
		\bigboxplus_{i \in I} p_i \cdot x_i = h \left(\sum_{i \in I} p_i \cdot \delta_{x_i}\right)
	$$
	for all finite $I$ and indexed collections $\{p_i\}_{i \in I}$, $\{x_i\}_{i \in I}$, such that $\sum_{i \in I} p_i \leq 1$ and for all $i \in I$, $x_i \in X$. This equips the set $X$ with a positive convex algebra structure.  
	\end{theorem}
	\begin{proof}
		See \cite[Theorem~4]{Jacobs:2010:Convexity} or \cite[Proposition~5.3]{Doberkat:2008:Erratum}.
	\end{proof}
	\section{Operational semantics}
	\section{Language semantics}

 