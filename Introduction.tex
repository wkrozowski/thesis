\chapter{Introduction}
\label{chapter:introduction}

One of the motivations for the mathematical study of the models of computation stems from the desire for precise and formal reasoning about the correctness of computer systems. These theoretical foundations enable formal verification experts to prove that systems deployed in safety-critical areas, such as avionics or healthcare, behave as expected. In Theoretical Computer Science it is customary to model computations as state transition systems, which are discrete models where a set of states is equipped with a notion of one-step observable behaviour, describing how the system evolves. Typical examples include automata, Kripke frames, and Markov chains among many others. 

Faced with a profusion of such objects, theoreticians have developed several paradigms concerning the study of transition systems throughout the years. The focus of this thesis is on the classical approach of providing specification languages for representing transition systems and logics for reasoning about the equivalence or similarity of systems represented by expressions of interest. 
\subsection{Regular Expressions and Deterministic Finite Automata}
A central exemplar of the paradigm outlined above is Regular Expressions. In his influential paper from 1956, Kleene introduced Deterministic Finite Automata (DFAs), which are the fundamental model of sequential deterministic computation. Each state of a DFA can be associated with a formal language, a collection of strings describing possible trajectories that one-step dynamics can emit starting from the state of interest. This yields a notion of semantic equivalence, meaning that we say two states are language equivalent if they denote the same language. In the same paper, Kleene proposed Regular Expressions, which are an algebraic specification language for DFAs, as well as proved that both approaches are equally expressive (up to language equivalence) through a result known nowadays as Kleeneâ€™s theorem. As an open problem, he left a completeness question: are there a finite number of rules that enable reasoning about the language equivalence of regular expressions? 

This kind of problem concerning specification languages has been of particular interest to theoretical computer scientists for decades. When a complete axiomatisation is available, one may reason about model behaviour through the syntactic manipulation of terms of the specification language, which is well-suited for implementation, automation, and formal reasoning.

In the case of Regular Expressions, Redko demonstrated that one cannot use a finite number of equational axioms to axiomatise the language equivalence. But the search for axiomatisation made of more expressive rules continued. The first answer came in 1966 from Salomaa, who presented two axiom systems. One was infinitary, and the other used finite equations along with an implicational rule. The latter became a blueprint for axiomatisations of semantic equivalence or similarity of transition systems.
\subsection{Axiomatic reasoning about transition systems}
Since Kleene, automata theorists have studied a multitude of models, including nondeterministic, weighted and probabilistic ones, usually focusing on the notion of language equivalence or inclusion. The advent of Process Algebra in the 80s shifted the perspective. New models, such as Labelled Transition Systems (LTSes), were introduced. Even though Nondeterministic Finite Automata (NFAs) are closely related to LTSes, the notions of automata theoretic equivalences are often too coarse for many situations in Concurrency Theory. In particular, the notion of a language of an NFA is of linear time; that is, it hides the moment of resolving nondeterministic choice from the external observer. Milner and Park introduced a branching-time notion of equivalence known as bisimilarity. In this concept, an external observer can see when and how nondeterminism is resolved, thus making it finer than language equivalence. 

Despite a change of focus, the paradigm of specification languages and axiomatisations remained. Milner noted that in the bisimilarity setting, Regular Expressions lose their expressiveness. Namely, there exist transition systems that cannot be specified with Kleene's syntax. The answer to that problem involved a more complex language with binders and a recursion operator. Milner provided a complete axiomatisation, following a pattern similar to Salomaa. Since then, process algebraists have been looking into various models, specification languages, and notions of equivalence or similarity within the paradigm of axiomatisations. 
\subsection{Behavioural distances}
In many contexts, especially when dealing with probabilistic or quantitative models, focusing on exact equivalence of behaviours is too restrictive. Instead, it is often more meaningful to measure how far apart the behaviours of two terms are. This has motivated the development of behavioural distances, which endow the state spaces of transition systems with


% This just dumps some pseudolatin in so you can see some text in place.
\blindtext
