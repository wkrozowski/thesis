\chapter{Introduction}
\label{chapter:introduction}
One of the motivations for the mathematical study of the models of computation stems from the desire for precise and formal reasoning about the correctness of computer systems. These theoretical foundations enable formal verification experts to prove that systems deployed in safety-critical areas, such as avionics or healthcare, behave as expected. In theoretical computer science it is customary to model computations as state transition systems, which are discrete models where a set of states is equipped with a notion of one-step observable behaviour, describing how the system evolves. Typical examples include finite automata, Kripke frames, and Markov chains among many others. 

	This central topic of this thesis are axiomatisations of behaviour of transition systems. By this we mean providing expression languages for representing the behaviour of transition systems and the study of formal systems for reasoning about equivalence or similarity of behaviours represented by expressions of the interest. 
	
	The interest in axiomatising behaviour of transition systems originates from the seminal work of Kleene on regular expressions~\cite{Kleene:1951:Representation}.  In his influential paper from 1951, Kleene introduced deterministic finite automata (DFAs), which are the fundamental model of sequential deterministic computations. Each state of a DFA can be associated with a formal language, a collection of strings that are accepted starting from a given state. This characterises an important class of formal languages, known as regular languages. Classically, two states are equivalent if they recognise the same language. In the same paper, Kleene proposed regular expressions, which are an algebraic specification language for DFAs and proved that both formalisms are equally expressive through a result known nowadays as Kleene’s theorem. As an open problem, he left a completeness question: are there a finite number of rules that enable reasoning about language equivalence of regular expressions? 

Shortly after Kleene's paper, Redko~\cite{Redko:1964:Defining} demonstrated that one cannot use a finite number of equational axioms to axiomatise the language equivalence. But the search for axiomatisation made of more expressive rules continued. The first answer came in 1966 from Salomaa~\cite{Salomaa:1966:Two}, who presented two axiom systems. One was infinitary, and the other used finite equations along with an implicational rule encoding Arden's lemma~\cite{Arden:1961:Delayed} for formal languages. While Salomaa's implicational axiomatisation later became a blueprint for inference systems for reasoning about semantic equivalence or similarity of transition systems, this formal system was not algebraic. Essentially, the implicational rule relied on the productivity side-condition called empty word property (EWP) that caused the resulting axiomatisation to be unsound under substitution of letters by arbitrary expressions. This problem has motivated several researchers including Conway~\cite{Conway:2012:Regular}, Krob~\cite{Krob:1990:Complete} and Boffa~\cite{Boffa:1990:Une} to pursue the problem of obtaining algebraic axiomatisation of language equivalence of DFAs, eventually leading to a celebrated completeness result of Kozen~\cite{Kozen:1994:Completeness}. The inference system of Kozen is known nowadays under the name Kleene Algebra (KA) and it forms a basis of several formal systems for equational reasoning about imperative programs~\cite{Kozen:1996:Kleene}, packet-passing software defined networks~\cite{Anderson:2014:NetKAT}, and concurrent programs~\cite{Kappe:2017:Concurrent,Wagemaker:2019:Completeness} among many others.

Since Kleene, automata theorists have studied many variants of automata, including nondeterministic~\cite{Rabin:1959:Finite}, weighted~\cite{Schutzenberger:1961:Definition} and probabilistic~\cite{Rabin:1963:Probabilistic} ones, usually focusing on the notion of language equivalence or inclusion. At the advent of process algebra in the 1980s, Milner and Park brought the concept of bisimilarity~\cite{Park:1981:Concurrency}, a notion of equivalence finer than language equivalence, that was motivated by the needs of the study of concurrency theory and models such as labelled transition systems (LTSs). Essentially, language equivalence is a linear-time notion, as it hides the precise moment of resolving nondeterministic choice from the external observer. At the same time, bisimilarity allows for a more fine-grained comparison of behaviours by looking at the exact moment of resolving the nondeterministic choice. 

In his seminal paper from 1984, Milner~\cite{Milner:1984:Complete} considered a variant of LTSs that he called charts and studied the associated problem of axiomatising bisimilarity of charts. Milner observed that while the syntax of regular expression can be used to specify behaviours of charts, it is not expressive, that is there exist behaviours that cannot be specified using Kleene's syntax. Instead, he proposed a more general language called algebra of regular behaviours (ARB) featuring binders, action prefixing, and a recursion operator. Milner provided a suitable generalisation of Salomaa's non-algebraic axiomatisation and demonstrated its soundness and completeness with respect to bisimilarity of charts.   

The completeness results of Salomaa, Kozen and Milner mentioned above are prototypical instances of the vast strain of research that has been of particular interest to theoretical computer scientists for decades. Given a transition system model and an associated notion of semantic equivalence, having a complete axiomatisation allows one to reason about model behaviour through the syntactic manipulation of terms of the specification language, which is well-suited for implementation, automation, and formal reasoning. Each time when the needs of modelling computer systems result in a new transition system model or an associated notion of semantic equivalence, it is natural to ask about the complete axiomatisation. 

This thesis provides contributions to the above outlined field of axiomatisations of behaviours of transition systems in two orthogonal directions.
\begin{enumerate}
	\item The first part of the thesis is concerned with the study of formal systems for quantitative reasoning about behavioural metrics, that replace conventional notions of behavioural equivalence with a quantitative measure of how close the behaviour of two states of transition systems is.
	\item The second part focuses on probabilistic transition systems and presents a sound and complete axiomatisation of language equivalence of behaviours specified through the syntax of Probabilistic Regular Expressions.
	\end{enumerate}	
We now provide a brief outline of each of these directions.
\section{Behavioural Metrics}
In many contexts, especially when dealing with probabilistic or quantitative models, focusing on exact equivalence of behaviours such as language equivalence or bisimilarity is too restrictive. A tiny perturbation in observed probability or weights of transition can deem two states inequivalent.  Instead, it is often more meaningful to measure how far apart the behaviours of two states are. 

This has motivated the development of behavioural metrics, which endow the state spaces of transition systems with (pseudo)metric spaces quantifying the dissimilarity of states. In such a setting, states at distance zero are not necessarily the same, but rather equivalent with respect to some classical notion of behavioural equivalence. In a nutshell, equipping transition systems with such a notion of distance crucially relies on the possibility of lifting the distance between the states to the distance on the one-step observable behaviour of the transition system.

Behavioural distances first appeared in the context of probabilistic transition systems~\cite{Desharnais:2004:Metrics,Breugel:2001:Towards}. Here, one-step observable behaviour forms a probability distribution. In such a setting, in order to lift distances from the state space to one-step observable behaviour, one can rely on classic Kantorovich lifting from transportation theory~\cite{Villani:2009:Optimal}. 

Behavioural distances are not limited to probabilistic or weighted systems; instead, they can be defined meaningfully for a variety of transition systems~\cite{Baldan:2018:Coalgebraic}. One of the simplest instances is deterministic finite automata, which can be equipped with a shortest-distinguishing-word distance~\cite{Bonchi:2018:UpTo}, where the longer the smallest word that can witness inequivalence of two states is, the closer the behaviour of compared states is. To illustrate that, given an alphabet $\Sigma = \{a\}$, we have that a state recognising the language $\{a,aa, aaa\}$ is closer to the one recognising $\{a,aa,aaa,aaa\}$ rather than $\{a\}$. 

So far, the study of axiomatisations of behavioural distances have mainly focused on concrete probabilistic cases~\cite{Bacci:2018:Bisimilarity,Bacci:2018:TV,Bacci:2018:Algebraic}. Axiomatisations of other important instances of behavioural distances are still underexplored. The main goal of the first part of this thesis is to initiate the study of axiomatisations and completeness problems for behavioural distances beyond the concrete probabilistic instances. Our starting point is the work of Bacci, Bacci, Larsen and Mardare~\cite{Bacci:2018:Bisimilarity}, who gave a sound and complete axiomatisation of branching-time behavioural distance of terms of a probabilistic process calculus.

\section{Probabilistic Language Equivalence}
In his seminal work from 1963, Rabin introduced Probabilistic Automata~\cite{Rabin:1963:Probabilistic}. This model captures the simple notion of randomised computation and acts as an acceptor for probabilistic languages. Under such semantics, each word over some fixed alphabet is associated with a weight from the unit interval capturing how likely the word is to be accepted. Throughout the years, Probabilistic Automata were deeply studied from an algorithmic point of view~\cite{Kiefer:2011:Language} that eventually enabled the development of practical verification tools for randomised programs~\cite{Kiefer:2012:APEX}. 

In the process algebra community, Larsen and Skou~\cite{Larsen:1991:Bisimulation} devised a notion of probabilistic bisimilarity, while Stark and Smolka~\cite{Stark:2000:Complete} provided a probabilistic process calculus featuring binders and a recursion operator and gave a sound and complete axiomatisation of probabilistic bisimilarity of terms of their calculus. The later work of Silva and Sokolova~\cite{Silva:2011:Sound} showed that one can extend Stark and Smolka's system with additional axioms characterising probabilistic language equivalence to obtain a complete axiomatisation of language equivalence.

While the result of Silva and Sokolova enables the use of the process algebraic syntax of Stark and Smolka for reasoning about probabilistic language equivalence, it is natural to ask if one could obtain a similar picture to the one for the usual language equivalence. That is, to devise a simpler, binder-free specification language in the style of Kleene's Regular Expressions and provide a more streamlined axiomatisation in the style of Salomaa.
	
	This problem is the central motivation for the second part of this thesis. One of the main inspirations for that comes from the probabilistic pattern matching community, where researchers already considered Regular Expression-like operations to specify probabilistic languages~\cite{Ross:2000:Probabilistic}. They did so by replacing the union of languages and Kleene's star from the usual Regular Expression with their probabilistic counterparts, which respectively can be seen as a convex combination and a form of the Bernoulli process. At the same time, the precise connection of such syntaxes to the transition systems model was under-explored~\cite{Beeh:2017:Transformations} and the topic of axiomatisation was not tackled at all. 


\section{Coalgebra}
In this thesis, we rely on the framework of universal coalgebra, which provides an abstract and uniform treatment of transition systems through the language of category theory. One of the main reasons why we do so, stems from the fact that coalgebraic framework allows is able to express behavioural metrics and probabilistic language equivalence through a unified formalism.

Generally speaking, transition systems can be seen as pairs consisting of a set of states and a transition function, mapping each state to its one-step behaviour. The coalgebraic outlook allows abstracting away the features of the one-step behaviour of the transition system, such as inputs, labels, nondeterminism, probability, and the like through the notion of a type, formally modelled as an endofunctor on the category of sets and functions. Given a type functor, one can uniformly instantiate abstract results concerning the transition systems of the interest.

	Under mild set-theoretic size constraints on the type functor, one can construct a final coalgebra, a canonical domain for behavioural equivalence. For simple systems without side effects, like deterministic automata, the final coalgebra is carried by the set of all formal languages over some alphabet and thus captures the usual notion of language equivalence. At the same time, for more complicated systems like LTSs or probabilistic transition systems featuring side-effects (formally modelled through the notion of a monad), coalgebraic behavioural equivalence coincides with bisimilarity. 
	
	Modelling finer notions of semantic equivalence can be phrased by changing the base category over which the type functor is defined to a more structured setting than just sets and functions. For example, one of the ways to model probabilistic language equivalence in the language of coalgebra is to work with coalgebras for an appropriate type functor over the category of positive convex algebras. In this category, the final coalgebra is precisely carried by the set of all probabilistic languages over some fixed alphabet. 	 

At the same time, the recent work on coalgebraic behavioural metrics provided a categorical generalisation of Kantorovich lifting to lifting endofunctors over the category of sets to the category of pseudometric spaces and nonexpansive maps between them. Final coalgebras for type functors obtained through such liftings come equipped with a pseudometric between behaviours. Such a coalgebraic outlook enabled to generalise the notion of behavioural metrics beyond probabilistic transition systems. 
\section{Overview of the thesis}
\subparagraph{Chapter Two: A Complete Quantitative Axiomatisations of Behavioural Distance of Regular Expressions}
\subparagraph{Chapter Three: A Diagrammatic Approach to Behavioural Distance of Nondeterministic Processes}
\subparagraph{Chapter Four: Completeness Theorem for Probabilistic Regular Expressions}



% This just dumps some pseudolatin in so you can see some text in place.

